{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for the BiLSTM CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies for EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,MaxPooling1D,Flatten,concatenate\n",
    "\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "#Importing the below block to display all outputs \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the training file\n",
    "df = pd.read_csv(\"ner_datasetreference.csv\", encoding='latin')\n",
    "df = df.fillna(method='ffill')\n",
    "df[\"Sentence #\"] = df[\"Sentence #\"].apply(lambda s: s[9:]).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Sentence #, Word, POS, Tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for empty rows\n",
    "df[df['Word'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary to replace these latin characters \n",
    "replacement_dict = {'ë':'e','ü':'u',\"\\xa0\":' ', 'é':'e', '\\x93':' ','\\x91':' ','\\x97':' ','\\x85':' ','\\x94':' ','ö':'o' ,'°':' ', \n",
    "                   '\\x92':' ','\\x96':' '}\n",
    "\n",
    "def cleanunicode(uncleanstring):\n",
    "    return re.sub(r'[\\xa0|é|ë|ü|\\x93|\\x91|\\x97|\\x85|\\x94|ö|°|\\x92|\\x96]', lambda m: replacement_dict.get(m.group()), uncleanstring)\n",
    "\n",
    "df['Word'] = df['Word'].apply(lambda x : cleanunicode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Sentence #, Word, POS, Tag]\n",
       "Index: []"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for empty rows\n",
    "df[df['Word'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word and tag is combined in a list for each sentence another list is consuructed\n",
    "df['combined']= df[['Word','Tag']].values.tolist()\n",
    "df = df.groupby(['Sentence #'])['combined'].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47959"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at the number of sentences\n",
    "trainSentences = df.tolist()\n",
    "len(trainSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train test split of 80:20 ration\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(trainSentences, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38367"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9592"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking the the length of samples sent for training and test\n",
    "len(X_train)\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['No', 'O'],\n",
       " ['official', 'O'],\n",
       " ['announcement', 'O'],\n",
       " ['has', 'O'],\n",
       " ['come', 'O'],\n",
       " ['from', 'O'],\n",
       " ['the', 'O'],\n",
       " ['Iraqi', 'B-gpe'],\n",
       " ['Special', 'O'],\n",
       " ['Tribunal', 'O'],\n",
       " ['in', 'O'],\n",
       " ['charge', 'O'],\n",
       " ['of', 'O'],\n",
       " ['the', 'O'],\n",
       " ['trials', 'O'],\n",
       " [',', 'O'],\n",
       " ['but', 'O'],\n",
       " ['officials', 'O'],\n",
       " ['close', 'O'],\n",
       " ['to', 'O'],\n",
       " ['the', 'O'],\n",
       " ['case', 'O'],\n",
       " ['said', 'O'],\n",
       " ['Friday', 'B-tim'],\n",
       " ['that', 'O'],\n",
       " ['Saddam', 'B-per'],\n",
       " ['Hussein', 'I-per'],\n",
       " ['will', 'O'],\n",
       " ['be', 'O'],\n",
       " ['tried', 'O'],\n",
       " ['for', 'O'],\n",
       " ['the', 'O'],\n",
       " ['1982', 'B-tim'],\n",
       " ['killing', 'O'],\n",
       " ['of', 'O'],\n",
       " ['dozens', 'O'],\n",
       " ['of', 'O'],\n",
       " ['residents', 'O'],\n",
       " ['of', 'O'],\n",
       " ['the', 'O'],\n",
       " ['town', 'O'],\n",
       " ['of', 'O'],\n",
       " ['Dujail', 'B-geo'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first sentence\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a list of characters in a word and appends that list to the training set as shown below\n",
    "def characterSplit(Sentences):\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        for j,data in enumerate(sentence):\n",
    "            chars = [c for c in data[0]]\n",
    "            Sentences[i][j] = [data[0],chars,data[1]]\n",
    "    return Sentences\n",
    "\n",
    "X_train = characterSplit(X_train)\n",
    "X_test = characterSplit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['No', ['N', 'o'], 'O'],\n",
       " ['official', ['o', 'f', 'f', 'i', 'c', 'i', 'a', 'l'], 'O'],\n",
       " ['announcement',\n",
       "  ['a', 'n', 'n', 'o', 'u', 'n', 'c', 'e', 'm', 'e', 'n', 't'],\n",
       "  'O'],\n",
       " ['has', ['h', 'a', 's'], 'O'],\n",
       " ['come', ['c', 'o', 'm', 'e'], 'O'],\n",
       " ['from', ['f', 'r', 'o', 'm'], 'O'],\n",
       " ['the', ['t', 'h', 'e'], 'O'],\n",
       " ['Iraqi', ['I', 'r', 'a', 'q', 'i'], 'B-gpe'],\n",
       " ['Special', ['S', 'p', 'e', 'c', 'i', 'a', 'l'], 'O'],\n",
       " ['Tribunal', ['T', 'r', 'i', 'b', 'u', 'n', 'a', 'l'], 'O'],\n",
       " ['in', ['i', 'n'], 'O'],\n",
       " ['charge', ['c', 'h', 'a', 'r', 'g', 'e'], 'O'],\n",
       " ['of', ['o', 'f'], 'O'],\n",
       " ['the', ['t', 'h', 'e'], 'O'],\n",
       " ['trials', ['t', 'r', 'i', 'a', 'l', 's'], 'O'],\n",
       " [',', [','], 'O'],\n",
       " ['but', ['b', 'u', 't'], 'O'],\n",
       " ['officials', ['o', 'f', 'f', 'i', 'c', 'i', 'a', 'l', 's'], 'O'],\n",
       " ['close', ['c', 'l', 'o', 's', 'e'], 'O'],\n",
       " ['to', ['t', 'o'], 'O'],\n",
       " ['the', ['t', 'h', 'e'], 'O'],\n",
       " ['case', ['c', 'a', 's', 'e'], 'O'],\n",
       " ['said', ['s', 'a', 'i', 'd'], 'O'],\n",
       " ['Friday', ['F', 'r', 'i', 'd', 'a', 'y'], 'B-tim'],\n",
       " ['that', ['t', 'h', 'a', 't'], 'O'],\n",
       " ['Saddam', ['S', 'a', 'd', 'd', 'a', 'm'], 'B-per'],\n",
       " ['Hussein', ['H', 'u', 's', 's', 'e', 'i', 'n'], 'I-per'],\n",
       " ['will', ['w', 'i', 'l', 'l'], 'O'],\n",
       " ['be', ['b', 'e'], 'O'],\n",
       " ['tried', ['t', 'r', 'i', 'e', 'd'], 'O'],\n",
       " ['for', ['f', 'o', 'r'], 'O'],\n",
       " ['the', ['t', 'h', 'e'], 'O'],\n",
       " ['1982', ['1', '9', '8', '2'], 'B-tim'],\n",
       " ['killing', ['k', 'i', 'l', 'l', 'i', 'n', 'g'], 'O'],\n",
       " ['of', ['o', 'f'], 'O'],\n",
       " ['dozens', ['d', 'o', 'z', 'e', 'n', 's'], 'O'],\n",
       " ['of', ['o', 'f'], 'O'],\n",
       " ['residents', ['r', 'e', 's', 'i', 'd', 'e', 'n', 't', 's'], 'O'],\n",
       " ['of', ['o', 'f'], 'O'],\n",
       " ['the', ['t', 'h', 'e'], 'O'],\n",
       " ['town', ['t', 'o', 'w', 'n'], 'O'],\n",
       " ['of', ['o', 'f'], 'O'],\n",
       " ['Dujail', ['D', 'u', 'j', 'a', 'i', 'l'], 'B-geo'],\n",
       " ['.', ['.'], 'O']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first sentence after character split\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loops create a set of unique tags and a dictionary of unique words\n",
    "labelSet = set()\n",
    "words = {}\n",
    "\n",
    "for dataset in [X_train, X_test]:\n",
    "    for sentence in dataset:\n",
    "        for token,char,label in sentence:\n",
    "            labelSet.add(label)\n",
    "            words[token.lower()] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-art',\n",
       " 'B-eve',\n",
       " 'B-geo',\n",
       " 'B-gpe',\n",
       " 'B-nat',\n",
       " 'B-org',\n",
       " 'B-per',\n",
       " 'B-tim',\n",
       " 'I-art',\n",
       " 'I-eve',\n",
       " 'I-geo',\n",
       " 'I-gpe',\n",
       " 'I-nat',\n",
       " 'I-org',\n",
       " 'I-per',\n",
       " 'I-tim',\n",
       " 'O'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of unique words in the dataset:  31802\n"
     ]
    }
   ],
   "source": [
    "print(\"No of unique words in the dataset: \", len(words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each label is given a unique index \n",
    "label2Idx = {}\n",
    "for label in labelSet:\n",
    "    label2Idx[label] = len(label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-geo': 0,\n",
       " 'B-nat': 1,\n",
       " 'I-per': 2,\n",
       " 'I-geo': 3,\n",
       " 'B-eve': 4,\n",
       " 'B-art': 5,\n",
       " 'B-per': 6,\n",
       " 'B-gpe': 7,\n",
       " 'O': 8,\n",
       " 'I-eve': 9,\n",
       " 'I-org': 10,\n",
       " 'I-tim': 11,\n",
       " 'I-gpe': 12,\n",
       " 'B-org': 13,\n",
       " 'I-nat': 14,\n",
       " 'I-art': 15,\n",
       " 'B-tim': 16}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating unique dictionary of NER tags and saving to json for future use\n",
    "label2Idx\n",
    "\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "\n",
    "with open('idx2Label_Glove.json', 'w') as fp:\n",
    "    json.dump(idx2Label, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom case loop for additional layer, added aditional cases apart from the  \n",
    "case2Idx = {'numeric': 0, 'allLower':1, 'allUpper':2, 'initialUpper':3, 'other':4, 'mainly_numeric':5, 'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numeric': 0,\n",
       " 'allLower': 1,\n",
       " 'allUpper': 2,\n",
       " 'initialUpper': 3,\n",
       " 'other': 4,\n",
       " 'mainly_numeric': 5,\n",
       " 'contains_digit': 6,\n",
       " 'PADDING_TOKEN': 7}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the cae embedded dictionary and one hot encoded vector\n",
    "case2Idx\n",
    "caseEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Glove 100D word embedding to fetch the word vectors\n",
    "word2Idx = {}\n",
    "wordEmbeddings = []\n",
    "\n",
    "fEmbeddings = open(\"embeddings/glove.6B.100d.txt\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below for loop loops through all the unique words from the dataset and checks if the word is in the glove vocab \n",
    "# if yes fetches the embedded vector if no UNKNOWNN_TOKEN will be used later to replace them\n",
    "for line in fEmbeddings:\n",
    "    split = line.strip().split(\" \")\n",
    "    word = split[0]\n",
    "   \n",
    "    if len(word2Idx) == 0: #Add padding+unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1) #Zero vector vor 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if split[0].lower() in words:  #words has all the unique words from the input dataset\n",
    "        vector = np.array([float(num) for num in split[1:]]) #Because the array is list of strings it converts into float\n",
    "        wordEmbeddings.append(vector) \n",
    "        word2Idx[split[0]] = len(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "deletedwords = {k:v for k,v in words.items() if k not in word2Idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'06-mar': True,\n",
       " '07-jun': True,\n",
       " 'british-sponsored': True,\n",
       " 'ping-kun': True,\n",
       " 'ghangzhou': True,\n",
       " 'esquisabel': True,\n",
       " 'urtuzaga': True,\n",
       " 'brain-wasting': True,\n",
       " 'oil-and-gas': True,\n",
       " \"al-madai'ni\": True,\n",
       " 'below-normal': True,\n",
       " 'abu-hafs': True,\n",
       " 'non-electrical': True,\n",
       " 'kentung': True,\n",
       " 'natama': True,\n",
       " 'high-seas': True,\n",
       " '8,00,000': True,\n",
       " 'british-dutch': True,\n",
       " 'mientes': True,\n",
       " 'dejarte': True,\n",
       " 'al-muasher': True,\n",
       " 'non-jordanians': True,\n",
       " 'mladjen': True,\n",
       " 'kenjic': True,\n",
       " 'ethiopian-born': True,\n",
       " 'al-bahlul': True,\n",
       " 'el-maan': True,\n",
       " 'froce': True,\n",
       " 'israel-syria': True,\n",
       " 'hamas-led': True,\n",
       " 'supamongkhon': True,\n",
       " '07-feb': True,\n",
       " 'arcega': True,\n",
       " 'uncolonized': True,\n",
       " 'al-ghad': True,\n",
       " 'corruption-related': True,\n",
       " 'recently-repaired': True,\n",
       " 'dong-young': True,\n",
       " 'six-nation': True,\n",
       " 'swear-in': True,\n",
       " '60-thousand': True,\n",
       " 'al-adwa': True,\n",
       " 'african-union': True,\n",
       " '745.71': True,\n",
       " '65.32': True,\n",
       " 'shakiso': True,\n",
       " 'admhaiyah': True,\n",
       " 'tachileik': True,\n",
       " '7,00,000': True,\n",
       " 'anti-malaria': True,\n",
       " 'egebank': True,\n",
       " 'campaign-finance': True,\n",
       " '8,462': True,\n",
       " 'al-sunna': True,\n",
       " 'yellow-shirted': True,\n",
       " 'el-mallahi': True,\n",
       " '21-jul': True,\n",
       " 'akre-bijeel': True,\n",
       " 'shaikan': True,\n",
       " 'dihok': True,\n",
       " 'shanghai-bound': True,\n",
       " 'nato-afghan': True,\n",
       " 'miltzow': True,\n",
       " 'zolfaghari': True,\n",
       " 'prison-like': True,\n",
       " 'military-owned': True,\n",
       " 'thousand-strong': True,\n",
       " 'el~universal': True,\n",
       " 'aleim': True,\n",
       " 'cease-fires': True,\n",
       " 'new~york': True,\n",
       " 'supporting-actress': True,\n",
       " 'supporting-actor': True,\n",
       " 'peace-broker': True,\n",
       " 'post-saddam': True,\n",
       " 'newly-built': True,\n",
       " 'mahee': True,\n",
       " '1,11,000': True,\n",
       " '2,00,000': True,\n",
       " 'stepped-up': True,\n",
       " 'farm-dependent': True,\n",
       " 'co-defendents': True,\n",
       " 'flavinols': True,\n",
       " 'newly-elected': True,\n",
       " 'french-colombian': True,\n",
       " 'kiryenko': True,\n",
       " 'member-nations': True,\n",
       " 'european-iranian': True,\n",
       " 're-arming': True,\n",
       " 'ching-hsi': True,\n",
       " 'non-opec': True,\n",
       " 'weisfield': True,\n",
       " 'speed-sensing': True,\n",
       " 'fixed-asset': True,\n",
       " 'koprowicz': True,\n",
       " 'al-ani': True,\n",
       " 'explosives-packed': True,\n",
       " 'maigao': True,\n",
       " 'structur': True,\n",
       " '2,000-years-old': True,\n",
       " 'majmaah': True,\n",
       " 'peformed': True,\n",
       " 'flood-swamped': True,\n",
       " 'football-shaped': True,\n",
       " 'trick-or-treaters': True,\n",
       " 'islamist-rooted': True,\n",
       " 'al-hamash': True,\n",
       " 'one-percent': True,\n",
       " '161-seat': True,\n",
       " '06-jul': True,\n",
       " '06-feb': True,\n",
       " 'al-qaida-linked': True,\n",
       " 'rashakai': True,\n",
       " 'gaming-related': True,\n",
       " 'ethnically-divided': True,\n",
       " 'al-zawahri': True,\n",
       " 'month-old': True,\n",
       " 'el-dabaa': True,\n",
       " '1,74,000': True,\n",
       " 'zero-interest': True,\n",
       " 'de~facto': True,\n",
       " '03-jan': True,\n",
       " '17-jun': True,\n",
       " 'douste-blazy': True,\n",
       " 'technology-intensive': True,\n",
       " 'plavia': True,\n",
       " '01-jun': True,\n",
       " 'al-jaafari': True,\n",
       " 'islamist-led': True,\n",
       " 'al-aqidi': True,\n",
       " 'dui-related': True,\n",
       " 'misno': True,\n",
       " 'three-decade-old': True,\n",
       " 'provincal': True,\n",
       " 'strongest-ever': True,\n",
       " 'tax-relief': True,\n",
       " 'csongrad': True,\n",
       " 'european-built': True,\n",
       " 'al-tahreer': True,\n",
       " 'un-cambodian': True,\n",
       " '2,20,000': True,\n",
       " 'flood-stricken': True,\n",
       " 'indian-based': True,\n",
       " 'free-enterprise': True,\n",
       " 'intelogic': True,\n",
       " '02-jun': True,\n",
       " 'rose-tinted': True,\n",
       " 'sub-commander': True,\n",
       " 'schwehm': True,\n",
       " 'abkhaz-controlled': True,\n",
       " 'oil-exporting': True,\n",
       " '3,00,000': True,\n",
       " 'chamrouen': True,\n",
       " 'then-tiny': True,\n",
       " 'closely-watched': True,\n",
       " 'tsunami-ravaged': True,\n",
       " 'ethnic-chinese': True,\n",
       " 'al-hayam': True,\n",
       " 'ipad-like': True,\n",
       " 'e2-k': True,\n",
       " 'estanged': True,\n",
       " 'kaibyshev': True,\n",
       " 'standard-of-living': True,\n",
       " 'kanyarukiga': True,\n",
       " 'frangiskos': True,\n",
       " '18,00': True,\n",
       " 'yeu-tzuoo': True,\n",
       " '06-apr': True,\n",
       " 'al-tayyah': True,\n",
       " 'stepstools': True,\n",
       " 'kalameh': True,\n",
       " 'hassem': True,\n",
       " 'unforseeable': True,\n",
       " 'non-peaceful': True,\n",
       " 'palestinian-owned': True,\n",
       " 'two-week-long': True,\n",
       " '5,00,000': True,\n",
       " 'alliot-marie': True,\n",
       " 'de-miners': True,\n",
       " \"shi'ite-led\": True,\n",
       " 'al-sabaa': True,\n",
       " 'sii-sii': True,\n",
       " '6,00,000': True,\n",
       " '05-feb': True,\n",
       " 'four-sets': True,\n",
       " '07-may': True,\n",
       " 'surfeited': True,\n",
       " 'recently-detained': True,\n",
       " 'bighi': True,\n",
       " 'henghameh': True,\n",
       " 'somaieh': True,\n",
       " 'matinpour': True,\n",
       " 'al-qaim': True,\n",
       " 'one-point-three': True,\n",
       " '30-year-long': True,\n",
       " 'orlandez': True,\n",
       " 'anti-monarchy': True,\n",
       " 'fulayfill': True,\n",
       " '1,30,000': True,\n",
       " 'pro-tibetan': True,\n",
       " 'counter-protesters': True,\n",
       " 'u.s-led': True,\n",
       " 'asset-valuation': True,\n",
       " 'weyn': True,\n",
       " 'al-muhajer': True,\n",
       " 'as-sahab': True,\n",
       " 'sinchulu': True,\n",
       " 'war-induced': True,\n",
       " 'export-driven': True,\n",
       " 'nina-maria': True,\n",
       " 'afghan-international': True,\n",
       " 'kountche': True,\n",
       " 'russia-backed': True,\n",
       " '2,11,000': True,\n",
       " '1,90,000': True,\n",
       " 'near-poor': True,\n",
       " 'al-mansoorain': True,\n",
       " 'guest-worker': True,\n",
       " 'nations-coordinated': True,\n",
       " 'vsv-30': True,\n",
       " 'hamas-ruled': True,\n",
       " 'liv-grete': True,\n",
       " 'second-': True,\n",
       " '1,40,000': True,\n",
       " 'sixth-straight': True,\n",
       " 'car-bomb': True,\n",
       " 'islamist-controlled': True,\n",
       " 'kilometer-per-hour': True,\n",
       " 'often-stalled': True,\n",
       " 'highly-indebted': True,\n",
       " 'mis-2009': True,\n",
       " 'super-giant': True,\n",
       " '350-58': True,\n",
       " \"baku-t'bilisi-ceyhan\": True,\n",
       " \"baku-t'bilisi-erzerum\": True,\n",
       " 'kars-akhalkalaki': True,\n",
       " 'al-sammarei': True,\n",
       " 'ex-dictator': True,\n",
       " 'trans-afghan': True,\n",
       " 'boodai': True,\n",
       " 'clunie-ross': True,\n",
       " 'triple-jump': True,\n",
       " '#name?': True,\n",
       " 'hurricane-': True,\n",
       " 'flood-ravaged': True,\n",
       " 'flakus': True,\n",
       " 'ghorian': True,\n",
       " 'lra..': True,\n",
       " 'rs-12m': True,\n",
       " 'state-nominee': True,\n",
       " 'six-to-three': True,\n",
       " 'po/psl': True,\n",
       " 'all-race': True,\n",
       " 'anti-insurgent': True,\n",
       " 'write-downs': True,\n",
       " '762.4': True,\n",
       " '752.2': True,\n",
       " 'henochowicz': True,\n",
       " 'mr.yushchenko': True,\n",
       " 'european-owned': True,\n",
       " 'al-uthmani': True,\n",
       " '77-year-old': True,\n",
       " 'terrorist-related': True,\n",
       " 'pakistan-bound': True,\n",
       " 'colombant': True,\n",
       " 'plastinina': True,\n",
       " 'mhawesh': True,\n",
       " 'snow-shortened': True,\n",
       " 'tsunami-devastated': True,\n",
       " 'integra-a': True,\n",
       " 'defacate': True,\n",
       " '7.00e+07': True,\n",
       " 'bankruptcy-law': True,\n",
       " 'haridy': True,\n",
       " 'arsamokov': True,\n",
       " 'kousalyan': True,\n",
       " 'prabtibha': True,\n",
       " 'guileful': True,\n",
       " 'anti-alliance': True,\n",
       " 'skubiszewski': True,\n",
       " '1,12,000': True,\n",
       " 'al-aalam': True,\n",
       " 'tahhar': True,\n",
       " 'kremlin-backed': True,\n",
       " 'long-successful': True,\n",
       " 'interlarded': True,\n",
       " 'minister-designate': True,\n",
       " 'khalidiyah': True,\n",
       " '03-jun': True,\n",
       " 'vasilily': True,\n",
       " 'filipchuk': True,\n",
       " '03-mar': True,\n",
       " 'group-a': True,\n",
       " '3,17,000': True,\n",
       " 'pro-syria': True,\n",
       " 'news-washington': True,\n",
       " 'dangabari': True,\n",
       " 'post-taleban': True,\n",
       " 'one-meter-long': True,\n",
       " 'anti-torture': True,\n",
       " '8,88,000': True,\n",
       " 'al-dulaimi': True,\n",
       " 'al-fagih': True,\n",
       " 'three-kilometer': True,\n",
       " 'soonthorn': True,\n",
       " 'rittipakdee': True,\n",
       " 'straight-set': True,\n",
       " 'jaish-e-mohammad': True,\n",
       " 'srinagar-based': True,\n",
       " 'half-english': True,\n",
       " '1,35,000': True,\n",
       " 'previously-unknown': True,\n",
       " 'storm-damaged': True,\n",
       " 'usaka': True,\n",
       " 'al-mukmin': True,\n",
       " 'euro-zone': True,\n",
       " 'sassou-nguesso': True,\n",
       " '121.79': True,\n",
       " 'baojing': True,\n",
       " 'jianhao': True,\n",
       " \"'t\": True,\n",
       " 'yousseff': True,\n",
       " 'boudhiba': True,\n",
       " 'tutsi-dominated': True,\n",
       " '1,10,000': True,\n",
       " 'massaki': True,\n",
       " 'bigger-than-expected': True,\n",
       " \"mutharika's\": True,\n",
       " 'serb-held': True,\n",
       " 'rain-swollen': True,\n",
       " '9,50,000': True,\n",
       " 'midestern': True,\n",
       " 'johnson-sirleaf': True,\n",
       " 'explosive-laden': True,\n",
       " 'first-trimester': True,\n",
       " 'botimar': True,\n",
       " 'asou': True,\n",
       " 'mezerhane': True,\n",
       " \"yar'adua\": True,\n",
       " 'still-classified': True,\n",
       " 'kandili': True,\n",
       " 'e.u': True,\n",
       " 'mad-cow': True,\n",
       " 'al-yamoun': True,\n",
       " 'lightly-populated': True,\n",
       " '21-month-old': True,\n",
       " 'endeavourundocked': True,\n",
       " 'al-kidwa': True,\n",
       " 'ratners': True,\n",
       " 'opposition-aligned': True,\n",
       " 'bird-lime': True,\n",
       " '780.47': True,\n",
       " 'taleban-led': True,\n",
       " 'al-jibouri': True,\n",
       " 'denmark-sweden': True,\n",
       " 'morocco-spain': True,\n",
       " 'suichuan': True,\n",
       " 'islamic-oriented': True,\n",
       " '02-jan': True,\n",
       " 'osbek': True,\n",
       " 'francisely': True,\n",
       " 'guocong': True,\n",
       " 'deshu': True,\n",
       " 'husseindoust': True,\n",
       " 'syvkovych': True,\n",
       " 'el-shater': True,\n",
       " 'akitani-bob': True,\n",
       " 'mughani': True,\n",
       " 'mar-93': True,\n",
       " 'moscow-backed': True,\n",
       " 'llasa': True,\n",
       " \"sa'adi\": True,\n",
       " 'ahmadi-nejad': True,\n",
       " 'pakistani-controlled': True,\n",
       " '1,72,000': True,\n",
       " 'chenembiri': True,\n",
       " 'iftars': True,\n",
       " 'explosives-laden': True,\n",
       " 'al-kharj': True,\n",
       " 'fedora-wearing': True,\n",
       " 'al-ahbash': True,\n",
       " 'pro-hezbollah': True,\n",
       " 'israel-hezbollah': True,\n",
       " 'endeavouris': True,\n",
       " 'ngwira': True,\n",
       " 'alishar': True,\n",
       " 'co-principality': True,\n",
       " '2.3-million-strong': True,\n",
       " '04-jun': True,\n",
       " 'cudgelling': True,\n",
       " '13,406': True,\n",
       " 'government-allied': True,\n",
       " 'crj-200s': True,\n",
       " 'pro-taliban': True,\n",
       " 'aid-to-lebanon': True,\n",
       " 'imf/eu/world': True,\n",
       " 'bank-arranged': True,\n",
       " 'four-nation': True,\n",
       " 'bi-khim': True,\n",
       " 'rahama': True,\n",
       " 'berkofsky': True,\n",
       " 'pakistani-sponsored': True,\n",
       " 'pakistani-ruled': True,\n",
       " 'abayi': True,\n",
       " 'two-bomb': True,\n",
       " 'al-libbi': True,\n",
       " '361.8': True,\n",
       " 'group-b': True,\n",
       " 'khamas': True,\n",
       " 'istanbul-based': True,\n",
       " 'mazlum-der': True,\n",
       " 'macedonian-american': True,\n",
       " 'taliban-linked': True,\n",
       " 'reutersnews': True,\n",
       " 'six-kilometers': True,\n",
       " 'anti-iraqi': True,\n",
       " 'jundollah': True,\n",
       " 'worse-than-usual': True,\n",
       " 'drone-fired': True,\n",
       " 'oil-drilling': True,\n",
       " 'el-bared': True,\n",
       " 'al-yousifi': True,\n",
       " 'bengali-speaking': True,\n",
       " 'al-naimi': True,\n",
       " 'cosily': True,\n",
       " 'tilemaker': True,\n",
       " ' ': True,\n",
       " '8,206': True,\n",
       " 'harkat-ul-zihad': True,\n",
       " 'ojedokun': True,\n",
       " '4,00,000': True,\n",
       " '68-year-old': True,\n",
       " 'ferencevych': True,\n",
       " 'war-shattered': True,\n",
       " 'liabedzka': True,\n",
       " 'cyanide-based': True,\n",
       " 'fatah-linked': True,\n",
       " 'presutti': True,\n",
       " '43nd': True,\n",
       " 'yuanmu': True,\n",
       " 'gas-based': True,\n",
       " 'hyper-inflation': True,\n",
       " 'mellar': True,\n",
       " 'chemet': True,\n",
       " 'aero-technology': True,\n",
       " 'ounion': True,\n",
       " 'minar-e-pakistan': True,\n",
       " '0.050474537': True,\n",
       " 'anti-al-qaida': True,\n",
       " 'amiin': True,\n",
       " 'lembe': True,\n",
       " \"shi'ite-dominated\": True,\n",
       " 'quarter-point': True,\n",
       " 'rain-swept': True,\n",
       " 'american-israel': True,\n",
       " 'pro-shiite': True,\n",
       " 'sabouri': True,\n",
       " '81-4': True,\n",
       " 'tawilla': True,\n",
       " 'border-crossing': True,\n",
       " 'sixth-seed': True,\n",
       " 'asylum-seeking': True,\n",
       " 'yankham': True,\n",
       " 'zulima': True,\n",
       " 'democratia': True,\n",
       " 'metullah': True,\n",
       " 'floruan': True,\n",
       " 'muteiry': True,\n",
       " '80-kilometer-long': True,\n",
       " 'military-police': True,\n",
       " 'el-leil': True,\n",
       " 'alzouma': True,\n",
       " \"shi'ite-majority\": True,\n",
       " '38.875': True,\n",
       " 'al-bolani': True,\n",
       " 'al-waili': True,\n",
       " 'underdocumented': True,\n",
       " 'taliban-led': True,\n",
       " 'al-wefaq': True,\n",
       " '04-feb': True,\n",
       " 'first-set': True,\n",
       " 'el~tiempo': True,\n",
       " 'butheetaung': True,\n",
       " 'explosives-filled': True,\n",
       " 'griffal': True,\n",
       " 'idolwinner': True,\n",
       " 'southern-based': True,\n",
       " 'ntawukulilyayo': True,\n",
       " 'gisagara': True,\n",
       " 'tile-maker': True,\n",
       " 'anti-india': True,\n",
       " 'woo-ik': True,\n",
       " 'briston-myers': True,\n",
       " 'single-dose': True,\n",
       " 'republican-led': True,\n",
       " 'amharic-language': True,\n",
       " 'one-billion-dollars': True,\n",
       " 'soviet-dominated': True,\n",
       " 'bird-flu': True,\n",
       " 'penjwin': True,\n",
       " 'yoadimadji': True,\n",
       " 'jean-constatin': True,\n",
       " 'kanow': True,\n",
       " 'gomoa': True,\n",
       " 'buduburam': True,\n",
       " 'straw-yard': True,\n",
       " 'pressewednesday': True,\n",
       " 'post-abc': True,\n",
       " 'eu-backed': True,\n",
       " 'drug-fueled': True,\n",
       " 'gaza-egypt': True,\n",
       " 'tamil-dominated': True,\n",
       " 'working-level': True,\n",
       " 'dabous': True,\n",
       " 'hycamtin': True,\n",
       " 'manshiyet': True,\n",
       " 'ex-generals': True,\n",
       " 'varema': True,\n",
       " 'previously-owned': True,\n",
       " 'tsunami-affected': True,\n",
       " 'el~fasher': True,\n",
       " 'al-shaalan': True,\n",
       " '85-years-old': True,\n",
       " 'daglas': True,\n",
       " 'tarmohammed': True,\n",
       " 'least-developed': True,\n",
       " 'five-nation': True,\n",
       " 'actress-singer': True,\n",
       " 'otalvaro': True,\n",
       " 'then-justice': True,\n",
       " 'sudan-chad': True,\n",
       " '3,30,000': True,\n",
       " 'erbamont': True,\n",
       " 'five-judge': True,\n",
       " 'enzos': True,\n",
       " '6,40,000': True,\n",
       " '05-apr': True,\n",
       " 'eparses': True,\n",
       " 'taaf': True,\n",
       " '71-person': True,\n",
       " 'lower-house': True,\n",
       " 'euro-denominated': True,\n",
       " 'parliament-in-exile': True,\n",
       " '10-jan': True,\n",
       " 'gaza-bound': True,\n",
       " 'self-ruled': True,\n",
       " 'concides': True,\n",
       " 'qutbi': True,\n",
       " 'nairobi-based': True,\n",
       " 'clerq': True,\n",
       " 'c.i.a': True,\n",
       " 'helmund': True,\n",
       " '1,20,000': True,\n",
       " 'u.s.-nigerian': True,\n",
       " 'six-to-11-year-old': True,\n",
       " 'hamsat': True,\n",
       " 'near-daily': True,\n",
       " 'then-democratic': True,\n",
       " '6,50,000': True,\n",
       " 'jean-tobie': True,\n",
       " 'baku-tbilisi-ceyhan': True,\n",
       " '1,50,000': True,\n",
       " 'umurabyo': True,\n",
       " 'yankees-orioles': True,\n",
       " 'fallouj': True,\n",
       " 'vanallen': True,\n",
       " 'smyr': True,\n",
       " 'al-momen': True,\n",
       " 'colder-than-normal': True,\n",
       " 'aulnay-sous-bois': True,\n",
       " 'madagonia': True,\n",
       " 'novakatka': True,\n",
       " 'novakatkan': True,\n",
       " 'soyuzcapsule': True,\n",
       " 'kameshli': True,\n",
       " 'anti-kurd': True,\n",
       " 'daynunay': True,\n",
       " 'rocket-launched': True,\n",
       " 'plea-bargain': True,\n",
       " 'norway-based': True,\n",
       " 'geo-services': True,\n",
       " 'hsann': True,\n",
       " '131-5': True,\n",
       " 'violence-free': True,\n",
       " 'hizb-e-islami': True,\n",
       " '5,19,000': True,\n",
       " 'el~paso': True,\n",
       " 'lawmakers-elect': True,\n",
       " '22.772': True,\n",
       " 'al-enezi': True,\n",
       " 'nestrenko': True,\n",
       " 'rooting-out': True,\n",
       " '1,38,000': True,\n",
       " 'yellow-shirts': True,\n",
       " 'china-africa': True,\n",
       " 'thankot': True,\n",
       " 'afghan-pakistani': True,\n",
       " 'pro-al-qaida': True,\n",
       " 'virus-sharing': True,\n",
       " 'syarhei': True,\n",
       " 'antonchyk': True,\n",
       " 'indian-controlled': True,\n",
       " '03-feb': True,\n",
       " 'syrian-backed': True,\n",
       " 'wolde-michael': True,\n",
       " 'meshesha': True,\n",
       " 'kohdamani': True,\n",
       " 'saroki': True,\n",
       " 'ecolog': True,\n",
       " 'walbrecher': True,\n",
       " 'sugar-based': True,\n",
       " '616-5': True,\n",
       " 'uranium-enrichment': True,\n",
       " 'freeskate': True,\n",
       " 'long-ailing': True,\n",
       " '86-year-old': True,\n",
       " '8,17,000': True,\n",
       " 'bittok': True,\n",
       " 'mahyadi': True,\n",
       " '212-to-206': True,\n",
       " 'libyan-owned': True,\n",
       " 'kyauk-ein-su': True,\n",
       " 'ebola-like': True,\n",
       " 'pugiliste': True,\n",
       " 'erdagi': True,\n",
       " 'contract-style': True,\n",
       " '25.03.05': True,\n",
       " 'pro-pakistan': True,\n",
       " 'account-holders': True,\n",
       " 'record-highs': True,\n",
       " '3,000-meters': True,\n",
       " 'power-': True,\n",
       " 'wealth-sharing': True,\n",
       " 'islamic-rooted': True,\n",
       " 'dragmulla': True,\n",
       " 'abdel-al': True,\n",
       " \"pe'at\": True,\n",
       " 'deputy-leader': True,\n",
       " 'israeli-based': True,\n",
       " 'eight-tenths': True,\n",
       " 'johnson-morris': True,\n",
       " 'insurgent-plagued': True,\n",
       " 'bank-': True,\n",
       " 'imf-led': True,\n",
       " '185-million': True,\n",
       " 'anti-aids': True,\n",
       " 'hurricane-damaged': True,\n",
       " 'berkovsky': True,\n",
       " 'housedog': True,\n",
       " 'sunni-ruled': True,\n",
       " 'h.w': True,\n",
       " 'petrasevic': True,\n",
       " 'taliban-style': True,\n",
       " 'sight-impaired': True,\n",
       " 'amerzeb': True,\n",
       " 'al-adamiya': True,\n",
       " 'two-hectare': True,\n",
       " 'military-installed': True,\n",
       " 'ethiopian-backed': True,\n",
       " 'enming': True,\n",
       " '119.48': True,\n",
       " 'dikweneh': True,\n",
       " 'berrones': True,\n",
       " 'osaid': True,\n",
       " 'al-falluji': True,\n",
       " 'pro-thaksin': True,\n",
       " 'shamsbari': True,\n",
       " 'abel-rahim': True,\n",
       " '8,40,000': True,\n",
       " 'clichy-sous-bois': True,\n",
       " 'echoupal': True,\n",
       " 'tan-yard': True,\n",
       " 'seoul-based': True,\n",
       " 'infromation': True,\n",
       " 'narim': True,\n",
       " 'karshi-khanabad': True,\n",
       " 'iranativu': True,\n",
       " 'mizhir': True,\n",
       " 'yousi': True,\n",
       " 'three-phased': True,\n",
       " 'above-market': True,\n",
       " 'arson-related': True,\n",
       " 'mukakibibi': True,\n",
       " 'indian-pakistani': True,\n",
       " 'anti-china': True,\n",
       " 'over-threw': True,\n",
       " 'eritrea-ethiopia': True,\n",
       " 'model-turned-activist': True,\n",
       " '100-thousand': True,\n",
       " 're-tried': True,\n",
       " 'frank-walter': True,\n",
       " 'deeply-flawed': True,\n",
       " 'irrawady': True,\n",
       " 'northern-based': True,\n",
       " 'gas-rich': True,\n",
       " 'palestinian-claimed': True,\n",
       " '239-million': True,\n",
       " 'naankuse': True,\n",
       " 'jolie-pitt': True,\n",
       " 'bad-mouthing': True,\n",
       " 'al-khaznawi': True,\n",
       " 'mazdzer': True,\n",
       " '0.161': True,\n",
       " 'quarter-million': True,\n",
       " '5.7-kilometer': True,\n",
       " 'vorotan': True,\n",
       " 'post-assassination': True,\n",
       " 'al-uloum': True,\n",
       " 'pre-judging': True,\n",
       " 'pro-taleban': True,\n",
       " 'kakooza': True,\n",
       " '2.08.50': True,\n",
       " '2.07.51': True,\n",
       " 'kissem': True,\n",
       " 'tchangai-walla': True,\n",
       " '3,25,000': True,\n",
       " 'military-to-military': True,\n",
       " 'caymanians': True,\n",
       " 'base-closing': True,\n",
       " ' s': True,\n",
       " 'habsadeh': True,\n",
       " 'alyaty': True,\n",
       " 'privately-owned': True,\n",
       " 'government-ordered': True,\n",
       " 'squid-fishing': True,\n",
       " 'chiapolino': True,\n",
       " 'near-monoply': True,\n",
       " 'chikunova': True,\n",
       " 'nano-technology': True,\n",
       " 'micro-miniature': True,\n",
       " 'kasami': True,\n",
       " 'bomb-shaped': True,\n",
       " 'indonesia-based': True,\n",
       " 'khawaza': True,\n",
       " 'kehla': True,\n",
       " 'meter-long': True,\n",
       " 'f-430': True,\n",
       " 're-imposition': True,\n",
       " 'kazemeini': True,\n",
       " 'volganeft-139': True,\n",
       " 'al-hassani': True,\n",
       " 'public-opinion': True,\n",
       " 'macrumors.com': True,\n",
       " 'goose-quills': True,\n",
       " '02-feb': True,\n",
       " 'iskander-e': True,\n",
       " 'iranian-backed': True,\n",
       " 'industry-backed': True,\n",
       " 'minia': True,\n",
       " 'oil-giant': True,\n",
       " '73,276': True,\n",
       " 'muslim-croat': True,\n",
       " 'crnogorac': True,\n",
       " 'mar-46': True,\n",
       " 'feb-45': True,\n",
       " 'pre-2007': True,\n",
       " 'kabungulu': True,\n",
       " 'abdul-mahdi': True,\n",
       " 'al-hashemi': True,\n",
       " 'hae-sung': True,\n",
       " 'sandbag-reinforced': True,\n",
       " 'muslim-inhabited': True,\n",
       " 'price-per-carat': True,\n",
       " 'six-carat': True,\n",
       " 'uranium-enriching': True,\n",
       " 'jamahiriyah': True,\n",
       " 'leonella': True,\n",
       " 'sgorbati': True,\n",
       " 'pro-secular': True,\n",
       " '@dalailama': True,\n",
       " 'gingl': True,\n",
       " 'anti-cocaine': True,\n",
       " 'tehrik-e-taliban': True,\n",
       " 'inforamtion': True,\n",
       " 'pan-africa': True,\n",
       " 'makubalo': True,\n",
       " 'less-visible': True,\n",
       " 'yoon-jeong': True,\n",
       " '282-4': True,\n",
       " 'rain-interrupted': True,\n",
       " 'anti-takeover': True,\n",
       " '06-jan': True,\n",
       " 'post-tsunami': True,\n",
       " 'ghorbanpour': True,\n",
       " 'moscow-led': True,\n",
       " '01-feb': True,\n",
       " 'thirty-one-year-old': True,\n",
       " 'aberrahman': True,\n",
       " 'communist-ruled': True,\n",
       " 'even-odd': True,\n",
       " 'yentai': True,\n",
       " 'nateghi': True,\n",
       " 'promarket': True,\n",
       " 'disapora': True,\n",
       " 'sid-ahmed': True,\n",
       " 'effah-apenteng': True,\n",
       " 'block-to-block': True,\n",
       " 'military-ruled': True,\n",
       " 'snow-packed': True,\n",
       " 'matesi': True,\n",
       " 'flu-related': True,\n",
       " 'radak': True,\n",
       " 'sunrise-to-sunset': True,\n",
       " 'ozlam': True,\n",
       " 'morsink': True,\n",
       " 'mohtarem': True,\n",
       " 'mosquito-born': True,\n",
       " '2,47,000': True,\n",
       " '1,70,000': True,\n",
       " 'year-earlier': True,\n",
       " 'abderrahaman': True,\n",
       " 'essebar': True,\n",
       " 'ekici': True,\n",
       " 'malawai': True,\n",
       " 'al-aqili': True,\n",
       " 'massaua': True,\n",
       " '.tv': True,\n",
       " 'pickle-fork': True,\n",
       " 'costeira': True,\n",
       " 'italian-run': True,\n",
       " 'jean-cyril': True,\n",
       " 'four-day-long': True,\n",
       " 'wayaobao': True,\n",
       " 'zichang': True,\n",
       " 'marmaraout': True,\n",
       " 'pro-reformist': True,\n",
       " 'kypriano': True,\n",
       " 'rezayee': True,\n",
       " 'al-siddiq': True,\n",
       " 'al-hadhar': True,\n",
       " 'palnoo': True,\n",
       " 'gaza-egyptian': True,\n",
       " 'norwegian-owned': True,\n",
       " 'one-meter': True,\n",
       " 'gabgbo': True,\n",
       " 'pro-whaling': True,\n",
       " 'gaza-based': True,\n",
       " 'maoist-called': True,\n",
       " 'european-brokered': True,\n",
       " 'news-agency': True,\n",
       " 'cuban-engineered': True,\n",
       " '13.625': True,\n",
       " 'non-sawang': True,\n",
       " 'europeanunion-russian': True,\n",
       " 'meterologists': True,\n",
       " 'multi-million-selling': True,\n",
       " 'petropavlosk-kamchatskii': True,\n",
       " 'strife-torn': True,\n",
       " 'pirate-infested': True,\n",
       " 'kilometers-per-hour': True,\n",
       " 'auction-off': True,\n",
       " 'anti-secessionist': True,\n",
       " \"n'zerekore\": True,\n",
       " 'light-water': True,\n",
       " 'al-nabaie': True,\n",
       " 'wide-opened': True,\n",
       " 'anti-poppy': True,\n",
       " '10-to-eight': True,\n",
       " \"qur'an\": True,\n",
       " 'bitarov': True,\n",
       " '0.068171296': True,\n",
       " '0.068217593': True,\n",
       " '09-aug': True,\n",
       " 'yekiti': True,\n",
       " 'maashouq': True,\n",
       " 'israeli-egyptian': True,\n",
       " 'u.n.-congolese': True,\n",
       " '13,543': True,\n",
       " 'tibetan-american': True,\n",
       " 'ruling-party': True,\n",
       " '03-may': True,\n",
       " 'fourth-seed': True,\n",
       " '05-jul': True,\n",
       " 'half-a-percent': True,\n",
       " 'shareman': True,\n",
       " 'iraq-style': True,\n",
       " 'housing-loan': True,\n",
       " 'ex-mexican': True,\n",
       " 'buffetings': True,\n",
       " '1,80,000': True,\n",
       " 'kyoto-style': True,\n",
       " 'demontrators': True,\n",
       " 'kilometer-long': True,\n",
       " '4,532': True,\n",
       " 'pekanbara': True,\n",
       " 'iraqi-led': True,\n",
       " 'late-morning': True,\n",
       " 'heavily-fortified': True,\n",
       " \"sa'dun\": True,\n",
       " 'hamduni': True,\n",
       " 'energy-producing': True,\n",
       " 'chancellor-designate': True,\n",
       " '12,012': True,\n",
       " '12,049': True,\n",
       " 'zalinge': True,\n",
       " 'tarsyuk': True,\n",
       " 'pul-i-charki': True,\n",
       " 'veligonda': True,\n",
       " 'meters-deep': True,\n",
       " 'gray-list': True,\n",
       " '09-feb': True,\n",
       " 'jalbire': True,\n",
       " 'peipah': True,\n",
       " 'shoot-on-sight': True,\n",
       " 'wait-and-see': True,\n",
       " 'green-card': True,\n",
       " 'pro-freedom': True,\n",
       " 'risk-sharing': True,\n",
       " 'saudi-syrian': True,\n",
       " 'cross-ethnic': True,\n",
       " 'al-haidari': True,\n",
       " 'qabail': True,\n",
       " 'muqudadiyah': True,\n",
       " 'politically-motivated': True,\n",
       " 'arab-african': True,\n",
       " 'yulija': True,\n",
       " 'al-hafidh': True,\n",
       " 'developing-world': True,\n",
       " 'four-month-old': True,\n",
       " 'israeli-controlled': True,\n",
       " 'video-teleconference': True,\n",
       " 'kallenberger': True,\n",
       " 'three-crown': True,\n",
       " 'xdr-tb': True,\n",
       " 'pakhtoonkhaw': True,\n",
       " 'fuel-making': True,\n",
       " 'long-armed': True,\n",
       " 'rain-flooded': True,\n",
       " 'andrhra': True,\n",
       " 'unicorp': True,\n",
       " 'greek-owned': True,\n",
       " 'khanty-mansiisk': True,\n",
       " '55-to-45': True,\n",
       " 'hpvs': True,\n",
       " 'cedatos-gallup': True,\n",
       " '14-kilometer-long': True,\n",
       " 'island-nation': True,\n",
       " 'roseneft': True,\n",
       " 'ex-yukos': True,\n",
       " 'pro-kurdish': True,\n",
       " 'somalia-linked': True,\n",
       " 'domestically-produced': True,\n",
       " '7,864': True,\n",
       " 'hagino': True,\n",
       " 'al-fayfi': True,\n",
       " 'mebki': True,\n",
       " 'egyptian-owned': True,\n",
       " 'newly-signed': True,\n",
       " 'chad-sudan': True,\n",
       " 'congratuatlions': True,\n",
       " 'borroto': True,\n",
       " '130-thousand': True,\n",
       " 'sistan-baluchestan': True,\n",
       " 'vezzaz': True,\n",
       " 'tintane': True,\n",
       " 'recently-appointed': True,\n",
       " 'cardio-circulatory': True,\n",
       " 'noranit': True,\n",
       " 'setabutr': True,\n",
       " 'shoot-down': True,\n",
       " 'four-thousand': True,\n",
       " 'insurgency-related': True,\n",
       " 'uribana': True,\n",
       " 'road-block': True,\n",
       " 'retrosi': True,\n",
       " 'pallipat': True,\n",
       " 'sainworla': True,\n",
       " 'multi-fiber': True,\n",
       " '..': True,\n",
       " 'pat-downs': True,\n",
       " 'jalalzadeh': True,\n",
       " '3,400-meter-high': True,\n",
       " 'credit-driven': True,\n",
       " 'haze-hit': True,\n",
       " 'guatica': True,\n",
       " 'agha-mohammadi': True,\n",
       " 'u.s.-coalition': True,\n",
       " 'wardheer': True,\n",
       " 'bancoro': True,\n",
       " 'ndimyake': True,\n",
       " 'mwakalyelye': True,\n",
       " 're-vote': True,\n",
       " 'talaeng': True,\n",
       " 'burma-thailand': True,\n",
       " 'mixed-team': True,\n",
       " 'komuri': True,\n",
       " 'gas-guzzling': True,\n",
       " 'el~arish': True,\n",
       " 'league-n': True,\n",
       " 'ilhami': True,\n",
       " 'gafir': True,\n",
       " '2,10,000': True,\n",
       " 'roed-larsen': True,\n",
       " 'karbouli': True,\n",
       " 'martinin': True,\n",
       " '4,20,000': True,\n",
       " 'numaniya': True,\n",
       " 'privately-held': True,\n",
       " 'ui-chun': True,\n",
       " 'compensatiion': True,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deletedwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PADDING_TOKEN': 0,\n",
       " 'UNKNOWN_TOKEN': 1,\n",
       " 'the': 2,\n",
       " ',': 3,\n",
       " '.': 4,\n",
       " 'of': 5,\n",
       " 'to': 6,\n",
       " 'and': 7,\n",
       " 'in': 8,\n",
       " 'a': 9,\n",
       " '\"': 10,\n",
       " \"'s\": 11,\n",
       " 'for': 12,\n",
       " '-': 13,\n",
       " 'that': 14,\n",
       " 'on': 15,\n",
       " 'is': 16,\n",
       " 'was': 17,\n",
       " 'said': 18,\n",
       " 'with': 19,\n",
       " 'he': 20,\n",
       " 'as': 21,\n",
       " 'it': 22,\n",
       " 'by': 23,\n",
       " 'at': 24,\n",
       " '(': 25,\n",
       " ')': 26,\n",
       " 'from': 27,\n",
       " 'his': 28,\n",
       " 'an': 29,\n",
       " 'be': 30,\n",
       " 'has': 31,\n",
       " 'are': 32,\n",
       " 'have': 33,\n",
       " 'but': 34,\n",
       " 'were': 35,\n",
       " 'not': 36,\n",
       " 'this': 37,\n",
       " 'who': 38,\n",
       " 'they': 39,\n",
       " 'had': 40,\n",
       " 'i': 41,\n",
       " 'which': 42,\n",
       " 'will': 43,\n",
       " 'their': 44,\n",
       " ':': 45,\n",
       " 'or': 46,\n",
       " 'its': 47,\n",
       " 'one': 48,\n",
       " 'after': 49,\n",
       " 'new': 50,\n",
       " 'been': 51,\n",
       " 'also': 52,\n",
       " 'we': 53,\n",
       " 'would': 54,\n",
       " 'two': 55,\n",
       " 'more': 56,\n",
       " \"'\": 57,\n",
       " 'first': 58,\n",
       " 'about': 59,\n",
       " 'up': 60,\n",
       " 'when': 61,\n",
       " 'year': 62,\n",
       " 'there': 63,\n",
       " 'all': 64,\n",
       " '--': 65,\n",
       " 'out': 66,\n",
       " 'she': 67,\n",
       " 'other': 68,\n",
       " 'people': 69,\n",
       " \"n't\": 70,\n",
       " 'her': 71,\n",
       " 'percent': 72,\n",
       " 'than': 73,\n",
       " 'over': 74,\n",
       " 'into': 75,\n",
       " 'last': 76,\n",
       " 'some': 77,\n",
       " 'government': 78,\n",
       " 'time': 79,\n",
       " '$': 80,\n",
       " 'you': 81,\n",
       " 'years': 82,\n",
       " 'if': 83,\n",
       " 'no': 84,\n",
       " 'world': 85,\n",
       " 'can': 86,\n",
       " 'three': 87,\n",
       " 'do': 88,\n",
       " ';': 89,\n",
       " 'president': 90,\n",
       " 'only': 91,\n",
       " 'state': 92,\n",
       " 'million': 93,\n",
       " 'could': 94,\n",
       " 'us': 95,\n",
       " 'most': 96,\n",
       " 'against': 97,\n",
       " 'u.s.': 98,\n",
       " 'so': 99,\n",
       " 'them': 100,\n",
       " 'what': 101,\n",
       " 'him': 102,\n",
       " 'united': 103,\n",
       " 'during': 104,\n",
       " 'before': 105,\n",
       " 'may': 106,\n",
       " 'since': 107,\n",
       " 'many': 108,\n",
       " 'while': 109,\n",
       " 'where': 110,\n",
       " 'states': 111,\n",
       " 'because': 112,\n",
       " 'now': 113,\n",
       " 'city': 114,\n",
       " 'made': 115,\n",
       " 'like': 116,\n",
       " 'between': 117,\n",
       " 'did': 118,\n",
       " 'just': 119,\n",
       " 'national': 120,\n",
       " 'day': 121,\n",
       " 'country': 122,\n",
       " 'under': 123,\n",
       " 'such': 124,\n",
       " 'second': 125,\n",
       " 'then': 126,\n",
       " 'company': 127,\n",
       " 'group': 128,\n",
       " 'any': 129,\n",
       " 'through': 130,\n",
       " 'china': 131,\n",
       " 'four': 132,\n",
       " 'being': 133,\n",
       " 'down': 134,\n",
       " 'war': 135,\n",
       " 'back': 136,\n",
       " 'off': 137,\n",
       " 'south': 138,\n",
       " 'american': 139,\n",
       " 'minister': 140,\n",
       " 'police': 141,\n",
       " 'well': 142,\n",
       " 'including': 143,\n",
       " 'team': 144,\n",
       " 'international': 145,\n",
       " 'week': 146,\n",
       " 'officials': 147,\n",
       " 'still': 148,\n",
       " 'both': 149,\n",
       " 'even': 150,\n",
       " 'high': 151,\n",
       " 'part': 152,\n",
       " 'told': 153,\n",
       " 'those': 154,\n",
       " 'end': 155,\n",
       " 'former': 156,\n",
       " 'these': 157,\n",
       " 'make': 158,\n",
       " 'billion': 159,\n",
       " 'work': 160,\n",
       " 'our': 161,\n",
       " 'home': 162,\n",
       " 'school': 163,\n",
       " 'party': 164,\n",
       " 'house': 165,\n",
       " 'old': 166,\n",
       " 'later': 167,\n",
       " 'get': 168,\n",
       " 'another': 169,\n",
       " 'tuesday': 170,\n",
       " 'news': 171,\n",
       " 'long': 172,\n",
       " 'five': 173,\n",
       " 'called': 174,\n",
       " '1': 175,\n",
       " 'wednesday': 176,\n",
       " 'military': 177,\n",
       " 'way': 178,\n",
       " 'used': 179,\n",
       " 'much': 180,\n",
       " 'next': 181,\n",
       " 'monday': 182,\n",
       " 'thursday': 183,\n",
       " 'friday': 184,\n",
       " 'game': 185,\n",
       " 'here': 186,\n",
       " '?': 187,\n",
       " 'should': 188,\n",
       " 'take': 189,\n",
       " 'very': 190,\n",
       " 'my': 191,\n",
       " 'north': 192,\n",
       " 'security': 193,\n",
       " 'season': 194,\n",
       " 'york': 195,\n",
       " 'how': 196,\n",
       " 'public': 197,\n",
       " 'early': 198,\n",
       " 'according': 199,\n",
       " 'several': 200,\n",
       " 'court': 201,\n",
       " 'say': 202,\n",
       " 'around': 203,\n",
       " 'foreign': 204,\n",
       " '10': 205,\n",
       " 'until': 206,\n",
       " 'set': 207,\n",
       " 'political': 208,\n",
       " 'says': 209,\n",
       " 'market': 210,\n",
       " 'however': 211,\n",
       " 'family': 212,\n",
       " 'life': 213,\n",
       " 'same': 214,\n",
       " 'general': 215,\n",
       " 'left': 216,\n",
       " 'good': 217,\n",
       " 'top': 218,\n",
       " 'university': 219,\n",
       " 'going': 220,\n",
       " 'number': 221,\n",
       " 'major': 222,\n",
       " 'known': 223,\n",
       " 'points': 224,\n",
       " 'won': 225,\n",
       " 'six': 226,\n",
       " 'month': 227,\n",
       " 'dollars': 228,\n",
       " 'bank': 229,\n",
       " '2': 230,\n",
       " 'iraq': 231,\n",
       " 'use': 232,\n",
       " 'members': 233,\n",
       " 'each': 234,\n",
       " 'area': 235,\n",
       " 'found': 236,\n",
       " 'official': 237,\n",
       " 'sunday': 238,\n",
       " 'place': 239,\n",
       " 'go': 240,\n",
       " 'based': 241,\n",
       " 'among': 242,\n",
       " 'third': 243,\n",
       " 'times': 244,\n",
       " 'took': 245,\n",
       " 'right': 246,\n",
       " 'days': 247,\n",
       " 'local': 248,\n",
       " 'economic': 249,\n",
       " 'countries': 250,\n",
       " 'see': 251,\n",
       " 'best': 252,\n",
       " 'report': 253,\n",
       " 'killed': 254,\n",
       " 'held': 255,\n",
       " 'business': 256,\n",
       " 'west': 257,\n",
       " 'does': 258,\n",
       " 'own': 259,\n",
       " '%': 260,\n",
       " 'came': 261,\n",
       " 'law': 262,\n",
       " 'months': 263,\n",
       " 'women': 264,\n",
       " \"'re\": 265,\n",
       " 'power': 266,\n",
       " 'think': 267,\n",
       " 'service': 268,\n",
       " 'children': 269,\n",
       " 'bush': 270,\n",
       " 'show': 271,\n",
       " '/': 272,\n",
       " 'help': 273,\n",
       " 'chief': 274,\n",
       " 'saturday': 275,\n",
       " 'system': 276,\n",
       " 'john': 277,\n",
       " 'support': 278,\n",
       " 'series': 279,\n",
       " 'play': 280,\n",
       " 'office': 281,\n",
       " 'following': 282,\n",
       " 'me': 283,\n",
       " 'meeting': 284,\n",
       " 'expected': 285,\n",
       " 'late': 286,\n",
       " 'washington': 287,\n",
       " 'games': 288,\n",
       " 'european': 289,\n",
       " 'league': 290,\n",
       " 'reported': 291,\n",
       " 'final': 292,\n",
       " 'added': 293,\n",
       " 'without': 294,\n",
       " 'british': 295,\n",
       " 'white': 296,\n",
       " 'history': 297,\n",
       " 'man': 298,\n",
       " 'men': 299,\n",
       " 'became': 300,\n",
       " 'want': 301,\n",
       " 'march': 302,\n",
       " 'case': 303,\n",
       " 'few': 304,\n",
       " 'run': 305,\n",
       " 'money': 306,\n",
       " 'began': 307,\n",
       " 'open': 308,\n",
       " 'name': 309,\n",
       " 'trade': 310,\n",
       " 'center': 311,\n",
       " '3': 312,\n",
       " 'israel': 313,\n",
       " 'oil': 314,\n",
       " 'too': 315,\n",
       " 'al': 316,\n",
       " 'film': 317,\n",
       " 'win': 318,\n",
       " 'led': 319,\n",
       " 'east': 320,\n",
       " 'central': 321,\n",
       " '20': 322,\n",
       " 'air': 323,\n",
       " 'come': 324,\n",
       " 'chinese': 325,\n",
       " 'town': 326,\n",
       " 'leader': 327,\n",
       " 'army': 328,\n",
       " 'line': 329,\n",
       " 'never': 330,\n",
       " 'little': 331,\n",
       " 'played': 332,\n",
       " 'prime': 333,\n",
       " 'death': 334,\n",
       " 'companies': 335,\n",
       " 'least': 336,\n",
       " 'put': 337,\n",
       " 'forces': 338,\n",
       " 'past': 339,\n",
       " 'de': 340,\n",
       " 'half': 341,\n",
       " 'june': 342,\n",
       " 'saying': 343,\n",
       " 'know': 344,\n",
       " 'federal': 345,\n",
       " 'french': 346,\n",
       " 'peace': 347,\n",
       " 'earlier': 348,\n",
       " 'capital': 349,\n",
       " 'force': 350,\n",
       " 'great': 351,\n",
       " 'union': 352,\n",
       " 'near': 353,\n",
       " 'released': 354,\n",
       " 'small': 355,\n",
       " 'department': 356,\n",
       " 'every': 357,\n",
       " 'health': 358,\n",
       " 'japan': 359,\n",
       " 'head': 360,\n",
       " 'ago': 361,\n",
       " 'night': 362,\n",
       " 'big': 363,\n",
       " 'cup': 364,\n",
       " 'election': 365,\n",
       " 'region': 366,\n",
       " 'director': 367,\n",
       " 'talks': 368,\n",
       " 'program': 369,\n",
       " 'far': 370,\n",
       " 'today': 371,\n",
       " 'statement': 372,\n",
       " 'july': 373,\n",
       " 'although': 374,\n",
       " 'district': 375,\n",
       " 'again': 376,\n",
       " 'born': 377,\n",
       " 'development': 378,\n",
       " 'leaders': 379,\n",
       " 'council': 380,\n",
       " 'close': 381,\n",
       " 'record': 382,\n",
       " 'along': 383,\n",
       " 'county': 384,\n",
       " 'france': 385,\n",
       " 'went': 386,\n",
       " 'point': 387,\n",
       " 'must': 388,\n",
       " 'spokesman': 389,\n",
       " 'your': 390,\n",
       " 'member': 391,\n",
       " 'plan': 392,\n",
       " 'financial': 393,\n",
       " 'april': 394,\n",
       " 'recent': 395,\n",
       " 'campaign': 396,\n",
       " 'become': 397,\n",
       " 'troops': 398,\n",
       " 'whether': 399,\n",
       " 'lost': 400,\n",
       " 'music': 401,\n",
       " '15': 402,\n",
       " 'got': 403,\n",
       " 'israeli': 404,\n",
       " '30': 405,\n",
       " 'need': 406,\n",
       " '4': 407,\n",
       " 'lead': 408,\n",
       " 'already': 409,\n",
       " 'russia': 410,\n",
       " 'though': 411,\n",
       " 'might': 412,\n",
       " 'free': 413,\n",
       " 'hit': 414,\n",
       " 'rights': 415,\n",
       " '11': 416,\n",
       " 'information': 417,\n",
       " 'away': 418,\n",
       " '12': 419,\n",
       " '5': 420,\n",
       " 'others': 421,\n",
       " 'control': 422,\n",
       " 'within': 423,\n",
       " 'large': 424,\n",
       " 'economy': 425,\n",
       " 'press': 426,\n",
       " 'agency': 427,\n",
       " 'water': 428,\n",
       " 'died': 429,\n",
       " 'career': 430,\n",
       " 'making': 431,\n",
       " '...': 432,\n",
       " 'deal': 433,\n",
       " 'attack': 434,\n",
       " 'side': 435,\n",
       " 'seven': 436,\n",
       " 'better': 437,\n",
       " 'less': 438,\n",
       " 'september': 439,\n",
       " 'once': 440,\n",
       " 'clinton': 441,\n",
       " 'main': 442,\n",
       " 'due': 443,\n",
       " 'committee': 444,\n",
       " 'building': 445,\n",
       " 'conference': 446,\n",
       " 'club': 447,\n",
       " 'january': 448,\n",
       " 'decision': 449,\n",
       " 'stock': 450,\n",
       " 'america': 451,\n",
       " 'given': 452,\n",
       " 'give': 453,\n",
       " 'often': 454,\n",
       " 'announced': 455,\n",
       " 'television': 456,\n",
       " 'industry': 457,\n",
       " 'order': 458,\n",
       " 'young': 459,\n",
       " \"'ve\": 460,\n",
       " 'palestinian': 461,\n",
       " 'age': 462,\n",
       " 'start': 463,\n",
       " 'administration': 464,\n",
       " 'russian': 465,\n",
       " 'prices': 466,\n",
       " 'round': 467,\n",
       " 'december': 468,\n",
       " 'nations': 469,\n",
       " \"'m\": 470,\n",
       " 'human': 471,\n",
       " 'india': 472,\n",
       " 'defense': 473,\n",
       " 'asked': 474,\n",
       " 'total': 475,\n",
       " 'october': 476,\n",
       " 'players': 477,\n",
       " 'bill': 478,\n",
       " 'important': 479,\n",
       " 'southern': 480,\n",
       " 'move': 481,\n",
       " 'fire': 482,\n",
       " 'population': 483,\n",
       " 'rose': 484,\n",
       " 'november': 485,\n",
       " 'include': 486,\n",
       " 'further': 487,\n",
       " 'nuclear': 488,\n",
       " 'street': 489,\n",
       " 'taken': 490,\n",
       " 'media': 491,\n",
       " 'different': 492,\n",
       " 'issue': 493,\n",
       " 'received': 494,\n",
       " 'secretary': 495,\n",
       " 'return': 496,\n",
       " 'college': 497,\n",
       " 'working': 498,\n",
       " 'community': 499,\n",
       " 'eight': 500,\n",
       " 'groups': 501,\n",
       " 'despite': 502,\n",
       " 'level': 503,\n",
       " 'largest': 504,\n",
       " 'whose': 505,\n",
       " 'attacks': 506,\n",
       " 'germany': 507,\n",
       " 'august': 508,\n",
       " 'change': 509,\n",
       " 'church': 510,\n",
       " 'nation': 511,\n",
       " 'german': 512,\n",
       " 'station': 513,\n",
       " 'london': 514,\n",
       " 'weeks': 515,\n",
       " 'having': 516,\n",
       " '18': 517,\n",
       " 'research': 518,\n",
       " 'black': 519,\n",
       " 'services': 520,\n",
       " 'story': 521,\n",
       " '6': 522,\n",
       " 'europe': 523,\n",
       " 'sales': 524,\n",
       " 'policy': 525,\n",
       " 'visit': 526,\n",
       " 'northern': 527,\n",
       " 'lot': 528,\n",
       " 'across': 529,\n",
       " 'per': 530,\n",
       " 'current': 531,\n",
       " 'board': 532,\n",
       " 'football': 533,\n",
       " 'ministry': 534,\n",
       " 'workers': 535,\n",
       " 'vote': 536,\n",
       " 'book': 537,\n",
       " 'fell': 538,\n",
       " 'seen': 539,\n",
       " 'role': 540,\n",
       " 'students': 541,\n",
       " 'shares': 542,\n",
       " 'iran': 543,\n",
       " 'process': 544,\n",
       " 'agreement': 545,\n",
       " 'quarter': 546,\n",
       " 'full': 547,\n",
       " 'match': 548,\n",
       " 'started': 549,\n",
       " 'growth': 550,\n",
       " 'yet': 551,\n",
       " 'moved': 552,\n",
       " 'possible': 553,\n",
       " 'western': 554,\n",
       " 'special': 555,\n",
       " '100': 556,\n",
       " 'plans': 557,\n",
       " 'interest': 558,\n",
       " 'behind': 559,\n",
       " 'strong': 560,\n",
       " 'england': 561,\n",
       " 'named': 562,\n",
       " 'food': 563,\n",
       " 'period': 564,\n",
       " 'real': 565,\n",
       " 'authorities': 566,\n",
       " 'car': 567,\n",
       " 'term': 568,\n",
       " 'rate': 569,\n",
       " 'race': 570,\n",
       " 'nearly': 571,\n",
       " 'korea': 572,\n",
       " 'enough': 573,\n",
       " 'site': 574,\n",
       " 'opposition': 575,\n",
       " 'keep': 576,\n",
       " '25': 577,\n",
       " 'call': 578,\n",
       " 'future': 579,\n",
       " 'taking': 580,\n",
       " 'island': 581,\n",
       " '2008': 582,\n",
       " '2006': 583,\n",
       " 'road': 584,\n",
       " 'outside': 585,\n",
       " 'really': 586,\n",
       " 'century': 587,\n",
       " 'democratic': 588,\n",
       " 'almost': 589,\n",
       " 'single': 590,\n",
       " 'share': 591,\n",
       " 'leading': 592,\n",
       " 'trying': 593,\n",
       " 'find': 594,\n",
       " 'album': 595,\n",
       " 'senior': 596,\n",
       " 'minutes': 597,\n",
       " 'together': 598,\n",
       " 'congress': 599,\n",
       " 'index': 600,\n",
       " 'australia': 601,\n",
       " 'results': 602,\n",
       " 'hard': 603,\n",
       " 'hours': 604,\n",
       " 'land': 605,\n",
       " 'action': 606,\n",
       " 'higher': 607,\n",
       " 'field': 608,\n",
       " 'cut': 609,\n",
       " 'coach': 610,\n",
       " 'elections': 611,\n",
       " 'san': 612,\n",
       " 'issues': 613,\n",
       " 'executive': 614,\n",
       " 'february': 615,\n",
       " 'production': 616,\n",
       " 'areas': 617,\n",
       " 'river': 618,\n",
       " 'face': 619,\n",
       " 'using': 620,\n",
       " 'japanese': 621,\n",
       " 'province': 622,\n",
       " 'park': 623,\n",
       " 'price': 624,\n",
       " 'commission': 625,\n",
       " 'california': 626,\n",
       " 'father': 627,\n",
       " 'son': 628,\n",
       " 'education': 629,\n",
       " '7': 630,\n",
       " 'village': 631,\n",
       " 'energy': 632,\n",
       " 'shot': 633,\n",
       " 'short': 634,\n",
       " 'africa': 635,\n",
       " 'key': 636,\n",
       " 'red': 637,\n",
       " 'association': 638,\n",
       " 'average': 639,\n",
       " 'pay': 640,\n",
       " 'exchange': 641,\n",
       " 'eu': 642,\n",
       " 'something': 643,\n",
       " 'gave': 644,\n",
       " 'likely': 645,\n",
       " 'player': 646,\n",
       " 'george': 647,\n",
       " '2007': 648,\n",
       " 'victory': 649,\n",
       " '8': 650,\n",
       " 'low': 651,\n",
       " 'things': 652,\n",
       " '2010': 653,\n",
       " 'pakistan': 654,\n",
       " '14': 655,\n",
       " 'post': 656,\n",
       " 'social': 657,\n",
       " 'continue': 658,\n",
       " 'ever': 659,\n",
       " 'look': 660,\n",
       " 'chairman': 661,\n",
       " 'job': 662,\n",
       " '2000': 663,\n",
       " 'soldiers': 664,\n",
       " 'able': 665,\n",
       " 'parliament': 666,\n",
       " 'front': 667,\n",
       " 'himself': 668,\n",
       " 'problems': 669,\n",
       " 'private': 670,\n",
       " 'lower': 671,\n",
       " 'list': 672,\n",
       " 'built': 673,\n",
       " '13': 674,\n",
       " 'efforts': 675,\n",
       " 'dollar': 676,\n",
       " 'miles': 677,\n",
       " 'included': 678,\n",
       " 'radio': 679,\n",
       " 'live': 680,\n",
       " 'form': 681,\n",
       " 'david': 682,\n",
       " 'african': 683,\n",
       " 'increase': 684,\n",
       " 'reports': 685,\n",
       " 'sent': 686,\n",
       " 'fourth': 687,\n",
       " 'always': 688,\n",
       " 'king': 689,\n",
       " '50': 690,\n",
       " 'tax': 691,\n",
       " 'taiwan': 692,\n",
       " 'britain': 693,\n",
       " '16': 694,\n",
       " 'playing': 695,\n",
       " 'title': 696,\n",
       " 'middle': 697,\n",
       " 'meet': 698,\n",
       " 'global': 699,\n",
       " 'wife': 700,\n",
       " '2009': 701,\n",
       " 'position': 702,\n",
       " 'located': 703,\n",
       " 'clear': 704,\n",
       " 'ahead': 705,\n",
       " '2004': 706,\n",
       " '2005': 707,\n",
       " 'iraqi': 708,\n",
       " 'english': 709,\n",
       " 'result': 710,\n",
       " 'release': 711,\n",
       " 'violence': 712,\n",
       " 'goal': 713,\n",
       " 'project': 714,\n",
       " 'closed': 715,\n",
       " 'border': 716,\n",
       " 'body': 717,\n",
       " 'soon': 718,\n",
       " 'crisis': 719,\n",
       " 'division': 720,\n",
       " 'served': 721,\n",
       " 'tour': 722,\n",
       " 'hospital': 723,\n",
       " 'kong': 724,\n",
       " 'test': 725,\n",
       " 'hong': 726,\n",
       " 'u.n.': 727,\n",
       " 'inc.': 728,\n",
       " 'technology': 729,\n",
       " 'believe': 730,\n",
       " 'organization': 731,\n",
       " 'published': 732,\n",
       " 'weapons': 733,\n",
       " 'agreed': 734,\n",
       " 'why': 735,\n",
       " 'nine': 736,\n",
       " 'summer': 737,\n",
       " 'wanted': 738,\n",
       " 'republican': 739,\n",
       " 'act': 740,\n",
       " 'recently': 741,\n",
       " 'texas': 742,\n",
       " 'course': 743,\n",
       " 'problem': 744,\n",
       " 'senate': 745,\n",
       " 'medical': 746,\n",
       " 'un': 747,\n",
       " 'done': 748,\n",
       " 'reached': 749,\n",
       " 'star': 750,\n",
       " 'continued': 751,\n",
       " 'investors': 752,\n",
       " 'living': 753,\n",
       " 'care': 754,\n",
       " 'signed': 755,\n",
       " '17': 756,\n",
       " 'art': 757,\n",
       " 'provide': 758,\n",
       " 'worked': 759,\n",
       " 'presidential': 760,\n",
       " 'gold': 761,\n",
       " 'obama': 762,\n",
       " 'morning': 763,\n",
       " 'dead': 764,\n",
       " 'opened': 765,\n",
       " \"'ll\": 766,\n",
       " 'event': 767,\n",
       " 'previous': 768,\n",
       " 'cost': 769,\n",
       " 'instead': 770,\n",
       " 'canada': 771,\n",
       " 'band': 772,\n",
       " 'teams': 773,\n",
       " 'daily': 774,\n",
       " '2001': 775,\n",
       " 'available': 776,\n",
       " 'drug': 777,\n",
       " 'coming': 778,\n",
       " '2003': 779,\n",
       " 'investment': 780,\n",
       " 'michael': 781,\n",
       " 'civil': 782,\n",
       " 'woman': 783,\n",
       " 'training': 784,\n",
       " 'appeared': 785,\n",
       " '9': 786,\n",
       " 'involved': 787,\n",
       " 'indian': 788,\n",
       " 'similar': 789,\n",
       " 'situation': 790,\n",
       " '24': 791,\n",
       " 'los': 792,\n",
       " 'running': 793,\n",
       " 'fighting': 794,\n",
       " 'mark': 795,\n",
       " '40': 796,\n",
       " 'trial': 797,\n",
       " 'hold': 798,\n",
       " 'australian': 799,\n",
       " 'thought': 800,\n",
       " '!': 801,\n",
       " 'study': 802,\n",
       " 'fall': 803,\n",
       " 'mother': 804,\n",
       " 'met': 805,\n",
       " 'relations': 806,\n",
       " '2002': 807,\n",
       " 'song': 808,\n",
       " 'popular': 809,\n",
       " 'base': 810,\n",
       " 'tv': 811,\n",
       " 'ground': 812,\n",
       " 'markets': 813,\n",
       " 'ii': 814,\n",
       " 'newspaper': 815,\n",
       " 'staff': 816,\n",
       " 'saw': 817,\n",
       " 'hand': 818,\n",
       " 'hope': 819,\n",
       " 'operations': 820,\n",
       " 'pressure': 821,\n",
       " 'americans': 822,\n",
       " 'eastern': 823,\n",
       " 'st.': 824,\n",
       " 'legal': 825,\n",
       " 'asia': 826,\n",
       " 'budget': 827,\n",
       " 'returned': 828,\n",
       " 'considered': 829,\n",
       " 'love': 830,\n",
       " 'wrote': 831,\n",
       " 'stop': 832,\n",
       " 'fight': 833,\n",
       " 'currently': 834,\n",
       " 'charges': 835,\n",
       " 'try': 836,\n",
       " 'aid': 837,\n",
       " 'ended': 838,\n",
       " 'management': 839,\n",
       " 'brought': 840,\n",
       " 'cases': 841,\n",
       " 'decided': 842,\n",
       " 'failed': 843,\n",
       " 'network': 844,\n",
       " 'works': 845,\n",
       " 'gas': 846,\n",
       " 'turned': 847,\n",
       " 'fact': 848,\n",
       " 'vice': 849,\n",
       " 'ca': 850,\n",
       " 'mexico': 851,\n",
       " 'trading': 852,\n",
       " 'especially': 853,\n",
       " 'reporters': 854,\n",
       " 'afghanistan': 855,\n",
       " 'common': 856,\n",
       " 'looking': 857,\n",
       " 'space': 858,\n",
       " 'rates': 859,\n",
       " 'manager': 860,\n",
       " 'loss': 861,\n",
       " '2011': 862,\n",
       " 'justice': 863,\n",
       " 'thousands': 864,\n",
       " 'james': 865,\n",
       " 'rather': 866,\n",
       " 'fund': 867,\n",
       " 'thing': 868,\n",
       " 'republic': 869,\n",
       " 'opening': 870,\n",
       " 'accused': 871,\n",
       " 'winning': 872,\n",
       " 'scored': 873,\n",
       " 'championship': 874,\n",
       " 'example': 875,\n",
       " 'getting': 876,\n",
       " 'biggest': 877,\n",
       " 'performance': 878,\n",
       " 'sports': 879,\n",
       " '1998': 880,\n",
       " 'let': 881,\n",
       " 'allowed': 882,\n",
       " 'schools': 883,\n",
       " 'means': 884,\n",
       " 'turn': 885,\n",
       " 'leave': 886,\n",
       " 'no.': 887,\n",
       " 'robert': 888,\n",
       " 'personal': 889,\n",
       " 'stocks': 890,\n",
       " 'showed': 891,\n",
       " 'light': 892,\n",
       " 'arrested': 893,\n",
       " 'person': 894,\n",
       " 'either': 895,\n",
       " 'offer': 896,\n",
       " 'majority': 897,\n",
       " 'battle': 898,\n",
       " '19': 899,\n",
       " 'class': 900,\n",
       " 'evidence': 901,\n",
       " 'makes': 902,\n",
       " 'society': 903,\n",
       " 'products': 904,\n",
       " 'regional': 905,\n",
       " 'needed': 906,\n",
       " 'stage': 907,\n",
       " 'am': 908,\n",
       " 'doing': 909,\n",
       " 'families': 910,\n",
       " 'construction': 911,\n",
       " 'various': 912,\n",
       " '1996': 913,\n",
       " 'sold': 914,\n",
       " 'independent': 915,\n",
       " 'kind': 916,\n",
       " 'airport': 917,\n",
       " 'paul': 918,\n",
       " 'judge': 919,\n",
       " 'internet': 920,\n",
       " 'movement': 921,\n",
       " 'room': 922,\n",
       " 'followed': 923,\n",
       " 'original': 924,\n",
       " 'angeles': 925,\n",
       " 'italy': 926,\n",
       " 'data': 927,\n",
       " 'comes': 928,\n",
       " 'parties': 929,\n",
       " 'nothing': 930,\n",
       " 'sea': 931,\n",
       " 'bring': 932,\n",
       " '2012': 933,\n",
       " 'annual': 934,\n",
       " 'officer': 935,\n",
       " 'beijing': 936,\n",
       " 'present': 937,\n",
       " 'remain': 938,\n",
       " 'nato': 939,\n",
       " '1999': 940,\n",
       " '22': 941,\n",
       " 'remains': 942,\n",
       " 'allow': 943,\n",
       " 'florida': 944,\n",
       " 'computer': 945,\n",
       " '21': 946,\n",
       " 'contract': 947,\n",
       " 'coast': 948,\n",
       " 'created': 949,\n",
       " 'demand': 950,\n",
       " 'operation': 951,\n",
       " 'events': 952,\n",
       " 'islamic': 953,\n",
       " 'beat': 954,\n",
       " 'analysts': 955,\n",
       " 'interview': 956,\n",
       " 'helped': 957,\n",
       " 'child': 958,\n",
       " 'probably': 959,\n",
       " 'spent': 960,\n",
       " 'asian': 961,\n",
       " 'effort': 962,\n",
       " 'cooperation': 963,\n",
       " 'shows': 964,\n",
       " 'calls': 965,\n",
       " 'investigation': 966,\n",
       " 'lives': 967,\n",
       " 'video': 968,\n",
       " 'yen': 969,\n",
       " 'runs': 970,\n",
       " 'tried': 971,\n",
       " 'bad': 972,\n",
       " 'described': 973,\n",
       " '1994': 974,\n",
       " 'toward': 975,\n",
       " 'written': 976,\n",
       " 'throughout': 977,\n",
       " 'established': 978,\n",
       " 'mission': 979,\n",
       " 'associated': 980,\n",
       " 'buy': 981,\n",
       " 'growing': 982,\n",
       " 'green': 983,\n",
       " 'forward': 984,\n",
       " 'competition': 985,\n",
       " 'poor': 986,\n",
       " 'latest': 987,\n",
       " 'banks': 988,\n",
       " 'question': 989,\n",
       " '1997': 990,\n",
       " 'prison': 991,\n",
       " 'feel': 992,\n",
       " 'attention': 993,\n",
       " 'themselves': 994,\n",
       " 'firm': 995,\n",
       " 'injured': 996,\n",
       " 'itself': 997,\n",
       " 'governor': 998,\n",
       " 'movie': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the word to index for future prediction use\n",
    "word2Idx\n",
    "idx2Word = {v: k for k, v in word2Idx.items()}\n",
    "with open('idx2Word_Glove.json', 'w') as fp:\n",
    "    json.dump(idx2Word, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.12729585,  0.03252421, -0.20221775, ..., -0.0215988 ,\n",
       "        -0.00363683,  0.20118529],\n",
       "       [-0.038194  , -0.24487   ,  0.72812   , ..., -0.1459    ,\n",
       "         0.8278    ,  0.27062   ],\n",
       "       ...,\n",
       "       [ 0.051214  ,  0.46039   ,  0.26446   , ...,  0.90017   ,\n",
       "        -0.019423  , -0.27108   ],\n",
       "       [ 0.089657  , -0.084513  , -0.056271  , ...,  0.65772   ,\n",
       "        -0.24955   , -0.23732   ],\n",
       "       [-0.013493  , -0.25268   , -0.5281    , ..., -0.10441   ,\n",
       "        -0.47526   , -0.56902   ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the list of list to array for model weights\n",
    "wordEmbeddings = np.array(wordEmbeddings)\n",
    "wordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'PADDING': 0,\n",
       " 'UNKNOWN': 1,\n",
       " ' ': 2,\n",
       " '0': 3,\n",
       " '1': 4,\n",
       " '2': 5,\n",
       " '3': 6,\n",
       " '4': 7,\n",
       " '5': 8,\n",
       " '6': 9,\n",
       " '7': 10,\n",
       " '8': 11,\n",
       " '9': 12,\n",
       " 'a': 13,\n",
       " 'b': 14,\n",
       " 'c': 15,\n",
       " 'd': 16,\n",
       " 'e': 17,\n",
       " 'f': 18,\n",
       " 'g': 19,\n",
       " 'h': 20,\n",
       " 'i': 21,\n",
       " 'j': 22,\n",
       " 'k': 23,\n",
       " 'l': 24,\n",
       " 'm': 25,\n",
       " 'n': 26,\n",
       " 'o': 27,\n",
       " 'p': 28,\n",
       " 'q': 29,\n",
       " 'r': 30,\n",
       " 's': 31,\n",
       " 't': 32,\n",
       " 'u': 33,\n",
       " 'v': 34,\n",
       " 'w': 35,\n",
       " 'x': 36,\n",
       " 'y': 37,\n",
       " 'z': 38,\n",
       " 'A': 39,\n",
       " 'B': 40,\n",
       " 'C': 41,\n",
       " 'D': 42,\n",
       " 'E': 43,\n",
       " 'F': 44,\n",
       " 'G': 45,\n",
       " 'H': 46,\n",
       " 'I': 47,\n",
       " 'J': 48,\n",
       " 'K': 49,\n",
       " 'L': 50,\n",
       " 'M': 51,\n",
       " 'N': 52,\n",
       " 'O': 53,\n",
       " 'P': 54,\n",
       " 'Q': 55,\n",
       " 'R': 56,\n",
       " 'S': 57,\n",
       " 'T': 58,\n",
       " 'U': 59,\n",
       " 'V': 60,\n",
       " 'W': 61,\n",
       " 'X': 62,\n",
       " 'Y': 63,\n",
       " 'Z': 64,\n",
       " '.': 65,\n",
       " ',': 66,\n",
       " '-': 67,\n",
       " '_': 68,\n",
       " '(': 69,\n",
       " ')': 70,\n",
       " '[': 71,\n",
       " ']': 72,\n",
       " '{': 73,\n",
       " '}': 74,\n",
       " '!': 75,\n",
       " '?': 76,\n",
       " ':': 77,\n",
       " ';': 78,\n",
       " '#': 79,\n",
       " \"'\": 80,\n",
       " '\"': 81,\n",
       " '/': 82,\n",
       " '\\\\': 83,\n",
       " '%': 84,\n",
       " '$': 85,\n",
       " '`': 86,\n",
       " '&': 87,\n",
       " '=': 88,\n",
       " '*': 89,\n",
       " '+': 90,\n",
       " '@': 91,\n",
       " '^': 92,\n",
       " '~': 93,\n",
       " '|': 94}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a character index dictionary of all possible english characters\n",
    "char2Idx = {\"PADDING\":0, \"UNKNOWN\":1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char2Idx[c] = len(char2Idx)\n",
    "len(char2Idx)\n",
    "char2Idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below function checks for a unique case of a word given from the predefined case rules and applies the specified case \n",
    "def getCasing(word, caseLookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    numDigits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            numDigits += 1\n",
    "            \n",
    "    digitFraction = numDigits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Is a digit\n",
    "        casing = 'numeric'\n",
    "    elif digitFraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower case\n",
    "        casing = 'allLower'\n",
    "    elif word.isupper(): #All upper case\n",
    "        casing = 'allUpper'\n",
    "    elif word[0].isupper(): #is a title, initial char upper, then all lower\n",
    "        casing = 'initialUpper'\n",
    "    elif numDigits > 0:\n",
    "        casing = 'contains_digit'\n",
    "    \n",
    "   \n",
    "    return caseLookup[casing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes the document and replaces the actual word/tag/case/character into its respective index\n",
    "def createMatrices(document, word2Idx, label2Idx, case2Idx,char2Idx):\n",
    "    unknownIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "    paddingIdx = word2Idx['PADDING_TOKEN']    \n",
    "        \n",
    "    dataset = []\n",
    "    \n",
    "    wordCount = 0\n",
    "    unknownWordCount = 0\n",
    "    \n",
    "    for document in document:\n",
    "        wordIndices = []    \n",
    "        caseIndices = []\n",
    "        charIndices = []\n",
    "        labelIndices = []\n",
    "        \n",
    "        for word,char,label in document:  \n",
    "            wordCount += 1\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]                 \n",
    "            else:\n",
    "                wordIdx = unknownIdx\n",
    "                unknownWordCount += 1\n",
    "            charIdx = []\n",
    "            for x in char:\n",
    "                charIdx.append(char2Idx[x])\n",
    "            #Get the label and map to int            \n",
    "            wordIndices.append(wordIdx)\n",
    "            caseIndices.append(getCasing(word, case2Idx))\n",
    "            charIndices.append(charIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "           \n",
    "        dataset.append([wordIndices, caseIndices, charIndices, labelIndices]) \n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function only adds padding to the character list of length 52 which represents the longest word\n",
    "def padding(document):\n",
    "    maxlen = 52 # using 52 which represents the longest word\n",
    "    for sentence in document:\n",
    "        char = sentence[2]\n",
    "        for x in char:\n",
    "            maxlen = max(maxlen,len(x))\n",
    "    for i,sentence in enumerate(document):\n",
    "        document[i][2] = pad_sequences(document[i][2],52,padding='post')\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above three functions are called on the train and test data\n",
    "train_set = padding(createMatrices(X_train,word2Idx,  label2Idx, case2Idx,char2Idx))\n",
    "test_set = padding(createMatrices(X_test, word2Idx, label2Idx, case2Idx,char2Idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function O/P the dataset and also o/p a list which contain the no of batches and its size\n",
    "def createBatches(data):\n",
    "    l = []\n",
    "    for i in data:\n",
    "        l.append(len(i[0]))\n",
    "    l = set(l)\n",
    "    batches = []\n",
    "    batch_len = []\n",
    "    z = 0\n",
    "    for i in l:\n",
    "        for batch in data:\n",
    "            if len(batch[0]) == i:\n",
    "                batches.append(batch)\n",
    "                z += 1\n",
    "        batch_len.append(z)\n",
    "    return batches,batch_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = characterSplit(trainSentences)\n",
    "full_train_data = padding(createMatrices(trainSentences,word2Idx,  label2Idx, case2Idx,char2Idx))\n",
    "full_train_data_batch,full_train_data_batch_len = createBatches(full_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the above function to get the train data and a train batch length to pass into model batch size which splits the \n",
    "# Train_batch/test_batch while fitting the model\n",
    "train_batch,train_batch_len = createBatches(train_set)\n",
    "test_batch,test_batch_len = createBatches(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38367"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at how many batches the data is split into\n",
    "len(train_set)\n",
    "len(train_batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THis function sorts the dataset by the length of sentences and then saves the length of each batch\n",
    "def iterate_minibatches(dataset,batch_len): \n",
    "    start = 0\n",
    "    for i in batch_len:\n",
    "        tokens = []\n",
    "        caseing = []\n",
    "        char = []\n",
    "        labels = []\n",
    "        data = dataset[start:i]\n",
    "        start = i\n",
    "        for dt in data:\n",
    "            t,c,ch,l = dt\n",
    "            l = np.expand_dims(l,-1)\n",
    "            tokens.append(t)\n",
    "            caseing.append(c)\n",
    "            char.append(ch)\n",
    "            labels.append(l)\n",
    "        yield np.asarray(labels),np.asarray(tokens),np.asarray(caseing),np.asarray(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters used base on the paper minor changes as \n",
    "# the dataset i used to train is very small compared to the dataset they used for training\n",
    "\n",
    "# EPOCHS = 50               # paper: 80\n",
    "# DROPOUT = 0.5             # paper: 0.68\n",
    "# DROPOUT_RECURRENT = 0.25  # not specified in paper, 0.25 recommended\n",
    "# LSTM_STATE_SIZE = 200     # paper: 275\n",
    "# CONV_SIZE = 3             # paper: 3\n",
    "# LEARNING_RATE = 0.0105    # paper 0.0105\n",
    "# OPTIMIZER = Nadam()       # paper uses SGD(lr=self.learning_rate), Nadam() recommended for smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_input (InputLayer)        [(None, None, 52)]   0           []                               \n",
      "                                                                                                  \n",
      " char_embedding (TimeDistribute  (None, None, 52, 30  2850       ['char_input[0][0]']             \n",
      " d)                             )                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 52, 30  0           ['char_embedding[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, None, 52, 30  2730       ['dropout[0][0]']                \n",
      " ted)                           )                                                                 \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, None, 1, 30)  0          ['time_distributed[0][0]']       \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " words_input (InputLayer)       [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " casing_input (InputLayer)      [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, None, 30)    0           ['time_distributed_1[0][0]']     \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 100)    2881700     ['words_input[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 8)      64          ['casing_input[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, None, 30)     0           ['time_distributed_2[0][0]']     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, None, 138)    0           ['embedding[0][0]',              \n",
      "                                                                  'embedding_1[0][0]',            \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, None, 400)    542400      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDistri  (None, None, 17)    6817        ['bidirectional[0][0]']          \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,436,561\n",
      "Trainable params: 554,797\n",
      "Non-trainable params: 2,881,764\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model layers \n",
    "\n",
    "#Word Embedding part\n",
    "words_input = Input(shape=(None,),dtype='int32',name='words_input')\n",
    "words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input)\n",
    "\n",
    "#Case embedding part of each word\n",
    "casing_input = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input)\n",
    "\n",
    "#Character embedding using 1D CNN\n",
    "character_input=Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out=TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input)\n",
    "dropout= Dropout(0.5)(embed_char_out)\n",
    "conv1d_out= TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout)\n",
    "maxpool_out=TimeDistributed(MaxPooling1D(52))(conv1d_out)\n",
    "char = TimeDistributed(Flatten())(maxpool_out)\n",
    "char = Dropout(0.5)(char)\n",
    "\n",
    "#Concatnating the embedded layer o/p together\n",
    "output = concatenate([words, casing, char])\n",
    "output = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output)\n",
    "output = TimeDistributed(Dense(len(label2Idx), activation='softmax'))(output)\n",
    "\n",
    "#Specifying input and o/p of the model\n",
    "model = Model(inputs=[words_input, casing_input, character_input], outputs=[output])\n",
    "\n",
    "#Compliling the model and looking at model summary\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50\n",
      "71/71 [==============================] - 167s 2s/step\n",
      " \n",
      "Epoch 1/50\n",
      "71/71 [==============================] - 149s 2s/step\n",
      " \n",
      "Epoch 2/50\n",
      "71/71 [==============================] - 150s 2s/step\n",
      " \n",
      "Epoch 3/50\n",
      "71/71 [==============================] - 164s 2s/step\n",
      " \n",
      "Epoch 4/50\n",
      "71/71 [==============================] - 166s 2s/step\n",
      " \n",
      "Epoch 5/50\n",
      "71/71 [==============================] - 166s 2s/step\n",
      " \n",
      "Epoch 6/50\n",
      "71/71 [==============================] - 168s 2s/step\n",
      " \n",
      "Epoch 7/50\n",
      "71/71 [==============================] - 170s 2s/step\n",
      " \n",
      "Epoch 8/50\n",
      "71/71 [==============================] - 168s 2s/step\n",
      " \n",
      "Epoch 9/50\n",
      "71/71 [==============================] - 167s 2s/step\n",
      " \n",
      "Epoch 10/50\n",
      "71/71 [==============================] - 170s 2s/step\n",
      " \n",
      "Epoch 11/50\n",
      "71/71 [==============================] - 165s 2s/step\n",
      " \n",
      "Epoch 12/50\n",
      "71/71 [==============================] - 171s 2s/step\n",
      " \n",
      "Epoch 13/50\n",
      "71/71 [==============================] - 172s 2s/step\n",
      " \n",
      "Epoch 14/50\n",
      "71/71 [==============================] - 169s 2s/step\n",
      " \n",
      "Epoch 15/50\n",
      "71/71 [==============================] - 171s 2s/step\n",
      " \n",
      "Epoch 16/50\n",
      "71/71 [==============================] - 163s 2s/step\n",
      " \n",
      "Epoch 17/50\n",
      "71/71 [==============================] - 184s 3s/step\n",
      " \n",
      "Epoch 18/50\n",
      "71/71 [==============================] - 176s 2s/step\n",
      " \n",
      "Epoch 19/50\n",
      "71/71 [==============================] - 174s 2s/step\n",
      " \n",
      "Epoch 20/50\n",
      "71/71 [==============================] - 175s 2s/step\n",
      " \n",
      "Epoch 21/50\n",
      "71/71 [==============================] - 175s 2s/step\n",
      " \n",
      "Epoch 22/50\n",
      "71/71 [==============================] - 176s 2s/step\n",
      " \n",
      "Epoch 23/50\n",
      "71/71 [==============================] - 175s 2s/step\n",
      " \n",
      "Epoch 24/50\n",
      "71/71 [==============================] - 178s 3s/step\n",
      " \n",
      "Epoch 25/50\n",
      "71/71 [==============================] - 176s 2s/step\n",
      " \n",
      "Epoch 26/50\n",
      "71/71 [==============================] - 178s 3s/step\n",
      " \n",
      "Epoch 27/50\n",
      "71/71 [==============================] - 177s 2s/step\n",
      " \n",
      "Epoch 28/50\n",
      "71/71 [==============================] - 178s 3s/step\n",
      " \n",
      "Epoch 29/50\n",
      "71/71 [==============================] - 176s 2s/step\n",
      " \n",
      "Epoch 30/50\n",
      "71/71 [==============================] - 175s 2s/step\n",
      " \n",
      "Epoch 31/50\n",
      "71/71 [==============================] - 182s 3s/step\n",
      " \n",
      "Epoch 32/50\n",
      "71/71 [==============================] - 180s 3s/step\n",
      " \n",
      "Epoch 33/50\n",
      "71/71 [==============================] - 179s 3s/step\n",
      " \n",
      "Epoch 34/50\n",
      "71/71 [==============================] - 177s 2s/step\n",
      " \n",
      "Epoch 35/50\n",
      "71/71 [==============================] - 178s 3s/step\n",
      " \n",
      "Epoch 36/50\n",
      "71/71 [==============================] - 177s 2s/step\n",
      " \n",
      "Epoch 37/50\n",
      "71/71 [==============================] - 176s 2s/step\n",
      " \n",
      "Epoch 38/50\n",
      "71/71 [==============================] - 175s 2s/step\n",
      " \n",
      "Epoch 39/50\n",
      "71/71 [==============================] - 164s 2s/step\n",
      " \n",
      "Epoch 40/50\n",
      "71/71 [==============================] - 176s 2s/step\n",
      " \n",
      "Epoch 41/50\n",
      "71/71 [==============================] - 177s 2s/step\n",
      " \n",
      "Epoch 42/50\n",
      "71/71 [==============================] - 176s 2s/step\n",
      " \n",
      "Epoch 43/50\n",
      "71/71 [==============================] - 177s 2s/step\n",
      " \n",
      "Epoch 44/50\n",
      "71/71 [==============================] - 175s 2s/step\n",
      " \n",
      "Epoch 45/50\n",
      "71/71 [==============================] - 175s 2s/step\n",
      " \n",
      "Epoch 46/50\n",
      "71/71 [==============================] - 174s 2s/step\n",
      " \n",
      "Epoch 47/50\n",
      "71/71 [==============================] - 163s 2s/step\n",
      " \n",
      "Epoch 48/50\n",
      "71/71 [==============================] - 178s 3s/step\n",
      " \n",
      "Epoch 49/50\n",
      "71/71 [==============================] - 177s 2s/step\n",
      " \n",
      "Wall time: 2h 23min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Training the model on batches for 50 epochs \n",
    "epochs = 50\n",
    "for epoch in range(epochs):    \n",
    "    print(\"Epoch %d/%d\"%(epoch,epochs))\n",
    "    a = Progbar(len(train_batch_len))\n",
    "    for i,batch in enumerate(iterate_minibatches(train_batch,train_batch_len)):\n",
    "        labels, tokens, casing, char = batch       \n",
    "        model.train_on_batch([tokens, casing, char], labels)\n",
    "        a.update(i)\n",
    "    a.update(i+1)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to pass each sentence in the test data and fetch the predicted o/p\n",
    "def tag_dataset(dataset):\n",
    "    sentences = []\n",
    "    correctLabels = []\n",
    "    predLabels = []\n",
    "    b = Progbar(len(dataset))\n",
    "    for i,data in enumerate(dataset):    \n",
    "        tokens, casing,char, labels = data\n",
    "        sentences.append(tokens)\n",
    "        tokens = np.asarray([tokens])     \n",
    "        casing = np.asarray([casing])\n",
    "        char = np.asarray([char])\n",
    "        pred = model.predict([tokens, casing,char], verbose=False)[0]   \n",
    "        pred = pred.argmax(axis=-1) #Predict the classes \n",
    "        correctLabels.append(labels)\n",
    "        predLabels.append(pred)\n",
    "        b.update(i)\n",
    "    b.update(i+1)\n",
    "    return predLabels, correctLabels, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9592/9592 [==============================] - 2119s 221ms/step\n"
     ]
    }
   ],
   "source": [
    "#Calling the above function\n",
    "predLabels, correctLabels, sentences = tag_dataset(test_batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the labels of index\n",
    "def pred2label(pred,diction):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            out_i.append(diction[str(p)])\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "\n",
    "pred_labels = pred2label(predLabels,idx2Label)\n",
    "true_labels = pred2label(correctLabels,idx2Label)\n",
    "actual_words = pred2label(sentences,idx2Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 75.5%\n",
      "precision_score: 74.1%\n",
      "recall_score: 77.0%\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(true_labels, pred_labels)))\n",
    "print(\"precision_score: {:.1%}\".format(precision_score(true_labels, pred_labels)))\n",
    "print(\"recall_score: {:.1%}\".format(recall_score(true_labels, pred_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " ...]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-eve',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'I-org',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'I-org',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'I-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-tim',\n",
       " 'I-tim',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-gpe',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-geo',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " 'B-per',\n",
       " 'I-per',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-org',\n",
       " 'O',\n",
       " ...]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a flat list to look at Classification report\n",
    "test_pred = [ j for i in pred_labels for j in i]\n",
    "true_lab = [ j for i in true_labels for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mr081025\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00         1\n",
      "       B-eve       0.22      0.76      0.34        17\n",
      "       B-geo       0.96      0.73      0.83      9917\n",
      "       B-gpe       0.91      0.94      0.92      3075\n",
      "       B-nat       0.24      0.25      0.24        40\n",
      "       B-org       0.52      0.79      0.63      2629\n",
      "       B-per       0.68      0.91      0.78      2513\n",
      "       B-tim       0.84      0.91      0.87      3707\n",
      "       I-art       0.04      0.67      0.07         3\n",
      "       I-eve       0.02      1.00      0.04         1\n",
      "       I-geo       0.82      0.58      0.68      2153\n",
      "       I-gpe       0.34      1.00      0.51        15\n",
      "       I-nat       0.00      0.00      0.00         0\n",
      "       I-org       0.57      0.74      0.65      2526\n",
      "       I-per       0.84      0.86      0.85      3311\n",
      "       I-tim       0.61      0.87      0.72       931\n",
      "           O       0.99      0.99      0.99    179236\n",
      "\n",
      "    accuracy                           0.96    210075\n",
      "   macro avg       0.50      0.71      0.54    210075\n",
      "weighted avg       0.97      0.96      0.96    210075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_pred, true_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           True \tPred\n",
      "\n",
      "------------------------------\n",
      "british        B-gpe\tB-gpe\n",
      "voters         O\tO\n",
      "are            O\tO\n",
      "casting        O\tO\n",
      "ballots        O\tO\n",
      "thursday       B-tim\tB-tim\n",
      "in             O\tO\n",
      "local          O\tO\n",
      "and            O\tO\n",
      "regional       O\tO\n",
      "elections      O\tO\n",
      "widely         O\tO\n",
      "seen           O\tO\n",
      "as             O\tO\n",
      "a              O\tO\n",
      "referendum     O\tO\n",
      "on             O\tO\n",
      "prime          B-per\tB-per\n",
      "minister       I-per\tO\n",
      "tony           I-per\tB-per\n",
      "blair          I-per\tI-per\n",
      "'s             O\tO\n",
      "decade         O\tO\n",
      "in             O\tO\n",
      "office         O\tO\n",
      ".              O\tO\n"
     ]
    }
   ],
   "source": [
    "print(\"{:15}{:5}\\t{}\\n\".format(\"Word\", \"True\", \"Pred\"))\n",
    "print(\"-\"*30)\n",
    "\n",
    "i = np.random.randint(0, len(actual_words))\n",
    "\n",
    "for (w, t, pred) in zip(actual_words[i], true_labels[i], pred_labels[i]):\n",
    "    print(\"{:15}{}\\t{}\".format(w, t, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"HalfTrainedModels/Half_Trained_model_glove.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_input (InputLayer)        [(None, None, 52)]   0           []                               \n",
      "                                                                                                  \n",
      " char_embedding (TimeDistribute  (None, None, 52, 30  2850       ['char_input[0][0]']             \n",
      " d)                             )                                                                 \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, None, 52, 30  0           ['char_embedding[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " time_distributed_8 (TimeDistri  (None, None, 52, 30  2730       ['dropout_4[0][0]']              \n",
      " buted)                         )                                                                 \n",
      "                                                                                                  \n",
      " time_distributed_9 (TimeDistri  (None, None, 1, 30)  0          ['time_distributed_8[0][0]']     \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " words_input (InputLayer)       [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " casing_input (InputLayer)      [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " time_distributed_10 (TimeDistr  (None, None, 30)    0           ['time_distributed_9[0][0]']     \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, None, 100)    2881700     ['words_input[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, None, 8)      64          ['casing_input[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, None, 30)     0           ['time_distributed_10[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, None, 138)    0           ['embedding_6[0][0]',            \n",
      "                                                                  'embedding_7[0][0]',            \n",
      "                                                                  'dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, None, 400)   542400      ['concatenate_2[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " time_distributed_11 (TimeDistr  (None, None, 17)    6817        ['bidirectional_2[0][0]']        \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,436,561\n",
      "Trainable params: 554,797\n",
      "Non-trainable params: 2,881,764\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Training on full dataset\n",
    "#Model layers \n",
    "\n",
    "#Word Embedding part\n",
    "words_input_full = Input(shape=(None,),dtype='int32',name='words_input')\n",
    "words_full = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input_full)\n",
    "\n",
    "#Case embedding part of each word\n",
    "casing_input_full = Input(shape=(None,), dtype='int32', name='casing_input')\n",
    "casing_full = Embedding(output_dim=caseEmbeddings.shape[1], input_dim=caseEmbeddings.shape[0], weights=[caseEmbeddings], trainable=False)(casing_input_full)\n",
    "\n",
    "#Character embedding using 1D CNN\n",
    "character_input_full = Input(shape=(None,52,),name='char_input')\n",
    "embed_char_out_full = TimeDistributed(Embedding(len(char2Idx),30,embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5)), name='char_embedding')(character_input_full)\n",
    "dropout_full = Dropout(0.5)(embed_char_out_full)\n",
    "conv1d_out_full = TimeDistributed(Conv1D(kernel_size=3, filters=30, padding='same',activation='tanh', strides=1))(dropout_full)\n",
    "maxpool_out_full =TimeDistributed(MaxPooling1D(52))(conv1d_out_full)\n",
    "char_full = TimeDistributed(Flatten())(maxpool_out_full)\n",
    "char_full = Dropout(0.5)(char_full)\n",
    "\n",
    "#Concatnating the embedded layer o/p together\n",
    "output_full = concatenate([words_full, casing_full, char_full])\n",
    "output_full = Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25))(output_full)\n",
    "output_full = TimeDistributed(Dense(len(label2Idx), activation='softmax'))(output_full)\n",
    "\n",
    "#Specifying input and o/p of the model\n",
    "Full_Trained_model_glove = Model(inputs=[words_input_full, casing_input_full, character_input_full], outputs=[output_full])\n",
    "\n",
    "#Compliling the model and looking at model summary\n",
    "Full_Trained_model_glove.compile(loss='sparse_categorical_crossentropy', optimizer='nadam')\n",
    "Full_Trained_model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSentences = characterSplit(trainSentences)\n",
    "full_train_data = padding(createMatrices(trainSentences ,word2Idx,  label2Idx, case2Idx, char2Idx))\n",
    "full_train_data_batch,full_train_data_batch_len = createBatches(full_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50\n",
      "71/71 [==============================] - 306s 4s/step\n",
      " \n",
      "Epoch 1/50\n",
      "71/71 [==============================] - 274s 4s/step\n",
      " \n",
      "Epoch 2/50\n",
      "71/71 [==============================] - 276s 4s/step\n",
      " \n",
      "Epoch 3/50\n",
      "71/71 [==============================] - 279s 4s/step\n",
      " \n",
      "Epoch 4/50\n",
      "71/71 [==============================] - 283s 4s/step\n",
      " \n",
      "Epoch 5/50\n",
      "71/71 [==============================] - 277s 4s/step\n",
      " \n",
      "Epoch 6/50\n",
      "71/71 [==============================] - 278s 4s/step\n",
      " \n",
      "Epoch 7/50\n",
      "71/71 [==============================] - 280s 4s/step\n",
      " \n",
      "Epoch 8/50\n",
      "71/71 [==============================] - 285s 4s/step\n",
      " \n",
      "Epoch 9/50\n",
      "71/71 [==============================] - 281s 4s/step\n",
      " \n",
      "Epoch 10/50\n",
      "71/71 [==============================] - 282s 4s/step\n",
      " \n",
      "Epoch 11/50\n",
      "71/71 [==============================] - 276s 4s/step\n",
      " \n",
      "Epoch 12/50\n",
      "71/71 [==============================] - 282s 4s/step\n",
      " \n",
      "Epoch 13/50\n",
      "71/71 [==============================] - 271s 4s/step\n",
      " \n",
      "Epoch 14/50\n",
      "71/71 [==============================] - 280s 4s/step\n",
      " \n",
      "Epoch 15/50\n",
      "71/71 [==============================] - 281s 4s/step\n",
      " \n",
      "Epoch 16/50\n",
      "71/71 [==============================] - 281s 4s/step\n",
      " \n",
      "Epoch 17/50\n",
      "71/71 [==============================] - 265s 4s/step\n",
      " \n",
      "Epoch 18/50\n",
      "71/71 [==============================] - 270s 4s/step\n",
      " \n",
      "Epoch 19/50\n",
      "71/71 [==============================] - 277s 4s/step\n",
      " \n",
      "Epoch 20/50\n",
      "71/71 [==============================] - 281s 4s/step\n",
      " \n",
      "Epoch 21/50\n",
      "71/71 [==============================] - 273s 4s/step\n",
      " \n",
      "Epoch 22/50\n",
      "71/71 [==============================] - 268s 4s/step\n",
      " \n",
      "Epoch 23/50\n",
      "71/71 [==============================] - 267s 4s/step\n",
      " \n",
      "Epoch 24/50\n",
      "71/71 [==============================] - 274s 4s/step\n",
      " \n",
      "Epoch 25/50\n",
      "71/71 [==============================] - 274s 4s/step\n",
      " \n",
      "Epoch 26/50\n",
      "71/71 [==============================] - 265s 4s/step\n",
      " \n",
      "Epoch 27/50\n",
      "71/71 [==============================] - 278s 4s/step\n",
      " \n",
      "Epoch 28/50\n",
      "71/71 [==============================] - 278s 4s/step\n",
      " \n",
      "Epoch 29/50\n",
      "71/71 [==============================] - 274s 4s/step\n",
      " \n",
      "Epoch 30/50\n",
      "71/71 [==============================] - 275s 4s/step\n",
      " \n",
      "Epoch 31/50\n",
      "71/71 [==============================] - 282s 4s/step\n",
      " \n",
      "Epoch 32/50\n",
      "71/71 [==============================] - 272s 4s/step\n",
      " \n",
      "Epoch 33/50\n",
      "71/71 [==============================] - 259s 4s/step\n",
      " \n",
      "Epoch 34/50\n",
      "71/71 [==============================] - 242s 3s/step\n",
      " \n",
      "Epoch 35/50\n",
      "71/71 [==============================] - 247s 3s/step\n",
      " \n",
      "Epoch 36/50\n",
      "71/71 [==============================] - 251s 4s/step\n",
      " \n",
      "Epoch 37/50\n",
      "71/71 [==============================] - 252s 4s/step\n",
      " \n",
      "Epoch 38/50\n",
      "71/71 [==============================] - 243s 3s/step\n",
      " \n",
      "Epoch 39/50\n",
      "71/71 [==============================] - 216s 3s/step\n",
      " \n",
      "Epoch 40/50\n",
      "71/71 [==============================] - 254s 4s/step\n",
      " \n",
      "Epoch 41/50\n",
      "71/71 [==============================] - 253s 4s/step\n",
      " \n",
      "Epoch 42/50\n",
      "71/71 [==============================] - 260s 4s/step\n",
      " \n",
      "Epoch 43/50\n",
      "71/71 [==============================] - 230s 3s/step\n",
      " \n",
      "Epoch 44/50\n",
      "71/71 [==============================] - 235s 3s/step\n",
      " \n",
      "Epoch 45/50\n",
      "71/71 [==============================] - 216s 3s/step\n",
      " \n",
      "Epoch 46/50\n",
      "71/71 [==============================] - 212s 3s/step\n",
      " \n",
      "Epoch 47/50\n",
      "71/71 [==============================] - 220s 3s/step\n",
      " \n",
      "Epoch 48/50\n",
      "71/71 [==============================] - 261s 4s/step\n",
      " \n",
      "Epoch 49/50\n",
      "71/71 [==============================] - 266s 4s/step\n",
      " \n",
      "Wall time: 3h 41min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Training the model on batches for 50 epochs \n",
    "epochs = 50\n",
    "for epoch in range(epochs):    \n",
    "    print(\"Epoch %d/%d\"%(epoch,epochs))\n",
    "    a = Progbar(len(full_train_data_batch_len))\n",
    "    for i,batch in enumerate(iterate_minibatches(full_train_data_batch,full_train_data_batch_len)):\n",
    "        labels, tokens, casing, char = batch       \n",
    "        Full_Trained_model_glove.train_on_batch([tokens, casing, char], labels)\n",
    "        a.update(i)\n",
    "    a.update(i+1)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_Trained_model_glove.save(\"TrainedModels/Full_Trained_model_glove.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on climate change data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "LoadedModel = keras.models.load_model(\"TrainedModels/Full_Trained_model_glove.h5\")\n",
    "\n",
    "#Reading the dictionaries back \n",
    "with open('idx2Word_Glove.json', 'r') as fp:\n",
    "    idx2Word = json.load(fp)\n",
    "    \n",
    "with open('idx2Label_Glove.json', 'r') as fp:\n",
    "    idx2Label = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanedTweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News Trends Data Americans are less concerned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you realize that civil war is the devastati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Having anxiety over the weather something they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the last few years I've noticed that studen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FULL INTERVIEW BTS ARMY BTSonGMA NEWS EXCLUSIV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       CleanedTweets\n",
       "0  News Trends Data Americans are less concerned ...\n",
       "1  Do you realize that civil war is the devastati...\n",
       "2  Having anxiety over the weather something they...\n",
       "3  In the last few years I've noticed that studen...\n",
       "4  FULL INTERVIEW BTS ARMY BTSonGMA NEWS EXCLUSIV..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1557690, 1)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Readig the cleaned dataset\n",
    "ClimateChange_DF = pd.read_csv(\"Cleaned_English_tweets.csv\")\n",
    "#keeping only the tweets\n",
    "ClimateChange_DF = ClimateChange_DF[[\"CleanedTweets\"]]\n",
    "ClimateChange_DF.head()\n",
    "ClimateChange_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting first 1000 tweets for ease of processing in next steps\n",
    "ClimateChange_DF_Sliced = ClimateChange_DF.iloc[:10000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClimateChange_DF_Sliced[\"Word\"] = ClimateChange_DF_Sliced[\"CleanedTweets\"].apply(lambda x: [[i] for i in str(x).split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CleanedTweets</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News Trends Data Americans are less concerned ...</td>\n",
       "      <td>[[News], [Trends], [Data], [Americans], [are],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you realize that civil war is the devastati...</td>\n",
       "      <td>[[Do], [you], [realize], [that], [civil], [war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Having anxiety over the weather something they...</td>\n",
       "      <td>[[Having], [anxiety], [over], [the], [weather]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the last few years I've noticed that studen...</td>\n",
       "      <td>[[In], [the], [last], [few], [years], [I've], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FULL INTERVIEW BTS ARMY BTSonGMA NEWS EXCLUSIV...</td>\n",
       "      <td>[[FULL], [INTERVIEW], [BTS], [ARMY], [BTSonGMA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       CleanedTweets  \\\n",
       "0  News Trends Data Americans are less concerned ...   \n",
       "1  Do you realize that civil war is the devastati...   \n",
       "2  Having anxiety over the weather something they...   \n",
       "3  In the last few years I've noticed that studen...   \n",
       "4  FULL INTERVIEW BTS ARMY BTSonGMA NEWS EXCLUSIV...   \n",
       "\n",
       "                                                Word  \n",
       "0  [[News], [Trends], [Data], [Americans], [are],...  \n",
       "1  [[Do], [you], [realize], [that], [civil], [war...  \n",
       "2  [[Having], [anxiety], [over], [the], [weather]...  \n",
       "3  [[In], [the], [last], [few], [years], [I've], ...  \n",
       "4  [[FULL], [INTERVIEW], [BTS], [ARMY], [BTSonGMA...  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ClimateChange_DF_Sliced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "climate_change = ClimateChange_DF_Sliced.Word.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the word into list of chatracters\n",
    "def FcharacterSplit(Sentences):\n",
    "    for i,sentence in enumerate(Sentences):\n",
    "        for j,data in enumerate(sentence):\n",
    "            chars = [c for c in data[0]]\n",
    "            Sentences[i][j] = [data[0],chars]\n",
    "    return Sentences\n",
    "\n",
    "predicting_sentence = FcharacterSplit(climate_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addiing a dummy tag to process easilly\n",
    "for i in predicting_sentence:\n",
    "    for j in i:\n",
    "        j.append('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "climateChange_pred_data = padding(createMatrices(predicting_sentence, word2Idx, label2Idx, case2Idx,char2Idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8902\n"
     ]
    }
   ],
   "source": [
    "i = np.random.randint(0, len(climateChange_pred_data))\n",
    "print(i)\n",
    "for j, data in enumerate(climateChange_pred_data[i:i+1]):\n",
    "    tokens, casing, char, labels = data\n",
    "    token = np.asarray([tokens])     \n",
    "    casing = np.asarray([casing])\n",
    "    char = np.asarray([char])\n",
    "    pred = LoadedModel.predict([token, casing,char], verbose=False)[0]   \n",
    "    pred = pred.argmax(axis=-1) #Predict the classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the labels of index\n",
    "def pred2label(pred,diction):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out.append(diction[str(pred_i)])\n",
    "    return out\n",
    "\n",
    "pred_labels = pred2label(pred,idx2Label)\n",
    "actual_words = pred2label(tokens,idx2Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Word  Passed_Word    Tag_Predicted\n",
      "------------------------------\n",
      "FULL           full           O\n",
      "INTERVIEW      interview      O\n",
      "BTS            UNKNOWN_TOKEN  O\n",
      "ARMY           army           I-org\n",
      "BTSonGMA       UNKNOWN_TOKEN  I-org\n",
      "NEWS           news           I-org\n",
      "EXCLUSIVE      exclusive      O\n",
      "sits           sits           O\n",
      "down           down           O\n",
      "with           with           O\n",
      "pop            pop            O\n",
      "superstars     UNKNOWN_TOKEN  O\n",
      "and            and            O\n",
      "South          south          O\n",
      "Korean         korean         B-gpe\n",
      "Pres           UNKNOWN_TOKEN  O\n",
      "Moon           moon           B-per\n",
      "Jaein          UNKNOWN_TOKEN  I-geo\n",
      "as             as             O\n",
      "they           they           O\n",
      "speak          speak          O\n",
      "on             on             O\n",
      "tackling       tackling       O\n",
      "tough          tough          O\n",
      "issues         issues         O\n",
      "from           from           O\n",
      "COVID          UNKNOWN_TOKEN  B-geo\n",
      "to             to             O\n",
      "climate        climate        O\n",
      "change         change         O\n",
      "BTS            UNKNOWN_TOKEN  B-org\n",
      "ARMY           army           I-org\n",
      "BTSonGMA       UNKNOWN_TOKEN  I-org\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">FULL INTERVIEW BTS ARMY BTSonGMA NEWS EXCLUSIVE sits down with pop superstars and \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    South Korean\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " Pres Moon Jaein as they speak on tackling tough issues from COVID to climate change BTS ARMY BTSonGMA </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"{:15}{:15}{}\".format(\"Original_Word\", \"Passed_Word\", \"Tag_Predicted\"))\n",
    "print(\"-\"*30)\n",
    "\n",
    "for (o, w, pred) in zip(ClimateChange_DF_Sliced.CleanedTweets[i].split(), actual_words, pred_labels):\n",
    "    print(\"{:15}{:15}{}\".format(o, w, pred))\n",
    "\n",
    "#Using Spacy to cross check for entities\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = nlp(ClimateChange_DF_Sliced.CleanedTweets[i])\n",
    "displacy.render(text, style = 'ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
